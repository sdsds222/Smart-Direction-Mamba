"""
å¢é‡è®­ç»ƒè„šæœ¬ - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ (å·²ä¼˜åŒ–ä¿å­˜é€»è¾‘)
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
import argparse
import os
from train import DirectionDatasetBERT#, list_model_files, select_model_interactive
from mamba_de import MambaDirectionEstimator


def fine_tune_model(base_model_path, new_data_path, config):
    """å¢é‡è®­ç»ƒ"""
    print("\n" + "="*70)
    print("ğŸ”„ å¢é‡è®­ç»ƒ (Fine-tuning)")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½å·²æœ‰æ¨¡å‹
    print(f"\n[1/5] åŠ è½½åŸºç¡€æ¨¡å‹: {os.path.basename(base_model_path)}")
    checkpoint = torch.load(base_model_path, map_location=device)
    
    model = MambaDirectionEstimator(
        vocab_size=checkpoint['vocab_size'],
        d_model=checkpoint['d_model'],
        d_state=checkpoint['d_state']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    
    original_val_acc = checkpoint.get('val_acc', 0.0) # ä½¿ç”¨ .get é¿å…æ—§æ¨¡å‹æ²¡è¿™ä¸ªkey
    print(f"âœ“ åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {original_val_acc:.2f}%")
    
    # 2. åŠ è½½æ–°æ•°æ®
    print(f"\n[2/5] åŠ è½½æ–°æ•°æ®...")
    dataset = DirectionDatasetBERT(
        csv_path=new_data_path,
        max_length=config['max_length'],
        use_bert_tokenizer=True
    )
    
    if len(dataset) < 10:
        print(f"âš ï¸  è­¦å‘Š: æ–°æ•°æ®é‡è¿‡å°‘ ({len(dataset)}æ¡)ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ã€‚")
        train_size = len(dataset)
        val_size = 0
    else:
        train_size = int(config['train_split'] * len(dataset))
        val_size = len(dataset) - train_size
        
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œval_loaderä¸ºç©º
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False) if val_size > 0 else None
    
    print(f"âœ“ æ–°è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"âœ“ æ–°éªŒè¯é›†: {val_size} æ ·æœ¬")
    
    # 3. è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼‰
    print(f"\n[3/5] é…ç½®ä¼˜åŒ–å™¨...")
    fine_tune_lr = config['learning_rate'] * 0.1  # å¾®è°ƒç”¨æ›´å°çš„å­¦ä¹ ç‡
    print(f"âœ“ å¾®è°ƒå­¦ä¹ ç‡: {fine_tune_lr} (åŸå§‹: {config['learning_rate']})")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=fine_tune_lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    criterion = nn.CrossEntropyLoss()
    
    # 4. å¢é‡è®­ç»ƒ
    print(f"\n[4/5] å¼€å§‹å¢é‡è®­ç»ƒ...")
    print("-"*70)
    
    best_finetune_acc = -1.0
    saved_model_path = ""
    
    os.makedirs('checkpoints', exist_ok=True)
    
    for epoch in range(config['num_epochs']):
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0
        
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['direction'].to(device)
            logits = model(input_ids)
            loss = criterion(logits, labels)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)
        
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100 * train_correct / train_total
        
        val_acc = 0.0
        if val_loader:
            model.eval()
            val_loss, val_correct, val_total = 0.0, 0, 0
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    labels = batch['direction'].to(device)
                    logits = model(input_ids)
                    loss = criterion(logits, labels)
                    val_loss += loss.item()
                    preds = torch.argmax(logits, dim=-1)
                    val_correct += (preds == labels).sum().item()
                    val_total += labels.size(0)
            avg_val_loss = val_loss / len(val_loader)
            val_acc = 100 * val_correct / val_total
            print(f'Epoch [{epoch+1}/{config["num_epochs"]}] Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        else:
            val_acc = train_acc
            print(f'Epoch [{epoch+1}/{config["num_epochs"]}] Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% | (æ— éªŒè¯é›†)')

        
        if val_acc > best_finetune_acc:
            best_finetune_acc = val_acc
            
            model_path_relative = f'checkpoints/{config["model_name"]}.pth'
            torch.save({
                'epoch': epoch, 'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 'val_acc': val_acc,
                'vocab_size': model.embedding.num_embeddings, 'd_model': model.d_model,
                'd_state': model.d_state, 'use_bert_tokenizer': dataset.use_bert_tokenizer,
                'config': config, 'base_model': base_model_path, 'fine_tuned': True
            }, model_path_relative)
            
            # ã€é‡è¦æ”¹åŠ¨ã€‘: è·å–å¹¶ä¿å­˜æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            saved_model_path = os.path.abspath(model_path_relative)
            print(f'  âœ“ ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)')
        
        scheduler.step()
    
    # 5. æ€»ç»“
    print(f"\n[5/5] å¢é‡è®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {original_val_acc:.2f}%")
    print(f"å¾®è°ƒåæœ€ä½³å‡†ç¡®ç‡: {best_finetune_acc:.2f}%")
    
    if best_finetune_acc > -1.0:
        improvement = best_finetune_acc - original_val_acc
        if improvement > 0.01:
             print(f"âœ… æå‡: +{improvement:.2f}%")
        elif improvement < -0.01:
             print(f"âš ï¸  ä¸‹é™: {improvement:.2f}% (å¯èƒ½æ–°æ•°æ®ä¸åŸæ•°æ®å·®å¼‚å¤§)")
        else:
             print(f"â†’ æŒå¹³")
        # ã€é‡è¦æ”¹åŠ¨ã€‘: æ‰“å°ç»å¯¹è·¯å¾„
        print(f"\nâœ“ å¾®è°ƒæ¨¡å‹å·²ä¿å­˜è‡³: {saved_model_path}")
    else:
        print("\nâœ— æœ¬æ¬¡å¾®è°ƒæœªäº§ç”Ÿæœ‰æ•ˆæ¨¡å‹ã€‚")

    print("="*70)


def main():
    parser = argparse.ArgumentParser(description='å¢é‡è®­ç»ƒ - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ')
    parser.add_argument('-m', '--model', type=str, required=True, help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('-i', '--input', type=str, required=True, help='æ–°è®­ç»ƒæ•°æ®CSV')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=15, help='è®­ç»ƒè½®æ•°ï¼ˆå¯é€‚å½“å¢åŠ ï¼‰')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡ï¼ˆä¼šè‡ªåŠ¨ç¼©å°10å€ï¼‰')
    parser.add_argument('--model-name', type=str, default='fine_tuned_model', help='ä¿å­˜åç§°')
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸ”„ Mamba Direction Estimator - å¢é‡è®­ç»ƒ")
    print("="*70)
    print("\nğŸ’¡ å¢é‡è®­ç»ƒè¯´æ˜:")
    print("  - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­å­¦ä¹ æ–°æ•°æ®")
    print("  - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼ˆåŸå§‹çš„10%ï¼‰")
    print("  - ä¿ç•™åŸæœ‰çŸ¥è¯†ï¼Œå­¦ä¹ æ–°æ¨¡å¼")
    print("="*70)
    
    base_model_path = args.model
    if not os.path.exists(base_model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ {base_model_path}")
        return
    
    if not os.path.exists(args.input):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ® {args.input}")
        return
    
    print(f"\nåŸºç¡€æ¨¡å‹: {os.path.basename(base_model_path)}")
    print(f"æ–°æ•°æ®: {os.path.basename(args.input)}")
    
    confirm = input("\næ˜¯å¦å¼€å§‹å¢é‡è®­ç»ƒ? [Y/n]: ").strip().lower()
    if confirm == 'n':
        print("å–æ¶ˆ")
        return
    
    try:
        base_checkpoint = torch.load(base_model_path, map_location='cpu')
        config = base_checkpoint['config']
        config['batch_size'] = args.batch_size
        config['num_epochs'] = args.epochs
        config['learning_rate'] = args.lr
        config['model_name'] = args.model_name
        print("\nâœ“ å·²ä»åŸºç¡€æ¨¡å‹åŠ è½½é…ç½®ï¼Œå¹¶ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ›´æ–°ã€‚")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•ä»æ—§æ¨¡å‹åŠ è½½é…ç½® ({e})ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
        config = {
            'max_length': 64, 'train_split': 0.8, 'batch_size': args.batch_size,
            'num_epochs': args.epochs, 'learning_rate': args.lr, 'model_name': args.model_name
        }

    try:
        fine_tune_model(base_model_path, args.input, config)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()

