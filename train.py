"""
äº¤äº’å¼è®­ç»ƒè„šæœ¬ - æ”¯æŒå¤šæ•°æ®é›†åˆå¹¶è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import numpy as np
import os
import argparse
import glob

try:
    from transformers import BertTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

from mamba_de import SimplifiedMambaSSM, MambaDirectionEstimator


def list_csv_files(directory='.'):
    """åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = glob.glob(os.path.join(directory, '*.csv'))
    return sorted(csv_files)


def select_multiple_csv(csv_files):
    """é€‰æ‹©å¤šä¸ªCSVæ–‡ä»¶"""
    print("\n" + "="*70)
    print("ğŸ“š å¤šæ•°æ®é›†é€‰æ‹©")
    print("="*70)
    print("\næç¤ºï¼šè¾“å…¥æ•°å­—åºå·ï¼Œç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”")
    print("ä¾‹å¦‚: 1,2,3 æˆ– 1 2 3")
    print("è¾“å…¥ 'all' é€‰æ‹©å…¨éƒ¨\n")
    
    for i, file in enumerate(csv_files, 1):
        size = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB, ~{total_rows} è¡Œ)")
        except:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB)")
    
    while True:
        selection = input(f"\nè¯·é€‰æ‹©è¦åˆå¹¶çš„æ•°æ®é›†: ").strip()
        
        if selection.lower() == 'all':
            return csv_files
        
        # è§£æè¾“å…¥
        try:
            # æ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš”
            if ',' in selection:
                indices = [int(x.strip()) for x in selection.split(',')]
            else:
                indices = [int(x.strip()) for x in selection.split()]
            
            # éªŒè¯ç´¢å¼•
            selected_files = []
            for idx in indices:
                if 1 <= idx <= len(csv_files):
                    selected_files.append(csv_files[idx-1])
                else:
                    print(f"âŒ åºå· {idx} æ— æ•ˆ")
                    break
            else:
                if selected_files:
                    print(f"\nâœ“ å·²é€‰æ‹© {len(selected_files)} ä¸ªæ•°æ®é›†:")
                    for f in selected_files:
                        print(f"  - {os.path.basename(f)}")
                    return selected_files
                else:
                    print("âŒ æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶")
        except ValueError:
            print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—åºå·")


def select_csv_interactive():
    """äº¤äº’å¼é€‰æ‹©CSVæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰"""
    print("\n" + "="*70)
    print("ğŸ“ é€‰æ‹©è®­ç»ƒæ•°æ®é›†")
    print("="*70)
    
    csv_files = list_csv_files()
    
    if not csv_files:
        print("âŒ å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        csv_path = input("\nè¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
        return [csv_path]
    
    print("\næ‰¾åˆ°ä»¥ä¸‹CSVæ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        size = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB, ~{total_rows} è¡Œ)")
        except:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB)")
    
    print(f"  {len(csv_files)+1}. æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„")
    print(f"  {len(csv_files)+2}. é€‰æ‹©å¤šä¸ªæ•°æ®é›†")
    
    while True:
        choice = input(f"\nè¯·é€‰æ‹© [1-{len(csv_files)+2}]: ").strip()
        
        if not choice.isdigit():
            print("âŒ è¯·è¾“å…¥æ•°å­—")
            continue
        
        choice = int(choice)
        
        if 1 <= choice <= len(csv_files):
            return [csv_files[choice-1]]
        elif choice == len(csv_files) + 1:
            csv_path = input("è¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(csv_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
                continue
            return [csv_path]
        elif choice == len(csv_files) + 2:
            return select_multiple_csv(csv_files)
        else:
            print(f"âŒ è¯·è¾“å…¥1-{len(csv_files)+2}ä¹‹é—´çš„æ•°å­—")


def merge_datasets(csv_paths):
    """åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶"""
    print("\n" + "="*70)
    print("ğŸ”„ åˆå¹¶æ•°æ®é›†")
    print("="*70)
    
    all_dfs = []
    total_samples = 0
    
    for csv_path in csv_paths:
        print(f"\nè¯»å–: {os.path.basename(csv_path)}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        if 'text' not in df.columns or 'direction' not in df.columns:
            print(f"  âš ï¸  è·³è¿‡: ç¼ºå°‘å¿…éœ€åˆ— (éœ€è¦ text å’Œ direction)")
            continue
        
        print(f"  âœ“ {len(df)} æ¡æ ·æœ¬")
        all_dfs.append(df)
        total_samples += len(df)
    
    if not all_dfs:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†å¯ä»¥åˆå¹¶")
    
    # åˆå¹¶
    merged_df = pd.concat(all_dfs, ignore_index=True)
    
    print(f"\nâœ“ åˆå¹¶å®Œæˆ: æ€»è®¡ {total_samples} æ¡æ ·æœ¬")
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†
    merged_path = 'merged_dataset.csv'
    merged_df.to_csv(merged_path, index=False, encoding='utf-8')
    print(f"âœ“ åˆå¹¶æ•°æ®é›†å·²ä¿å­˜è‡³: {merged_path}")
    
    return merged_path


def preview_dataset(csv_paths):
    """é¢„è§ˆæ•°æ®é›†ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªï¼‰"""
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é›†é¢„è§ˆ")
    print("="*70)
    
    # å¦‚æœæ˜¯å¤šä¸ªæ–‡ä»¶ï¼Œå…ˆæ˜¾ç¤ºå„è‡ªçš„ä¿¡æ¯
    if len(csv_paths) > 1:
        print(f"\nå…±é€‰æ‹© {len(csv_paths)} ä¸ªæ•°æ®é›†:")
        
        total_samples = 0
        all_directions = []
        
        for i, csv_path in enumerate(csv_paths, 1):
            df = pd.read_csv(csv_path, encoding='utf-8')
            print(f"\n{i}. {os.path.basename(csv_path)}")
            print(f"   æ ·æœ¬æ•°: {len(df)}")
            
            if 'direction' in df.columns:
                direction_counts = df['direction'].value_counts()
                print(f"   æ–¹å‘åˆ†å¸ƒ: ", end='')
                print(', '.join([f"{d}:{c}" for d, c in direction_counts.items()]))
                all_directions.extend(df['direction'].tolist())
            
            total_samples += len(df)
        
        print(f"\n{'='*70}")
        print(f"åˆå¹¶åç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        
        if all_directions:
            direction_counts = pd.Series(all_directions).value_counts()
            print(f"  æ–¹å‘åˆ†å¸ƒ:")
            for direction, count in direction_counts.items():
                pct = 100 * count / len(all_directions)
                print(f"    {direction}: {count} ({pct:.1f}%)")
            
            # æ£€æŸ¥å¹³è¡¡æ€§
            max_ratio = direction_counts.max() / direction_counts.min()
            if max_ratio > 3:
                print(f"  âš ï¸  æ•°æ®ä¸å¹³è¡¡ï¼ˆæœ€å¤§/æœ€å° = {max_ratio:.2f}ï¼‰")
        
        if total_samples < 50:
            print(f"  âš ï¸  æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼ˆå»ºè®®è‡³å°‘100æ¡ï¼‰")
        
        return True
    
    # å•ä¸ªæ–‡ä»¶çš„é¢„è§ˆ
    csv_path = csv_paths[0]
    df = pd.read_csv(csv_path, encoding='utf-8')
    
    print(f"\næ–‡ä»¶: {os.path.basename(csv_path)}")
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"åˆ—å: {list(df.columns)}")
    
    if 'text' not in df.columns:
        print("\nâš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° 'text' åˆ—")
        return False
    
    if 'direction' not in df.columns:
        print("\nâš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° 'direction' åˆ—")
        return False
    
    print("\næ–¹å‘åˆ†å¸ƒ:")
    direction_counts = df['direction'].value_counts()
    for direction, count in direction_counts.items():
        pct = 100 * count / len(df)
        print(f"  {direction}: {count} ({pct:.1f}%)")
    
    max_ratio = direction_counts.max() / direction_counts.min()
    if max_ratio > 3:
        print(f"\nâš ï¸  è­¦å‘Š: æ•°æ®ä¸å¹³è¡¡ï¼ˆæœ€å¤§/æœ€å° = {max_ratio:.2f}ï¼‰")
    
    if len(df) < 50:
        print(f"\nâš ï¸  è­¦å‘Š: æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼ˆå»ºè®®è‡³å°‘100æ¡ï¼‰")
    
    print("\nå‰3æ¡æ ·æœ¬:")
    print("-"*70)
    for i, row in df.head(3).iterrows():
        text = str(row['text'])[:50]
        direction = row['direction']
        print(f"{i+1}. [{direction}] {text}...")
    
    return True


def get_training_config():
    """äº¤äº’å¼è·å–è®­ç»ƒé…ç½®"""
    print("\n" + "="*70)
    print("âš™ï¸  è®­ç»ƒé…ç½®")
    print("="*70)
    
    configs = {}
    
    print("\n1. æ¨¡å‹å‚æ•°")
    configs['d_model'] = int(input("  åµŒå…¥ç»´åº¦ [é»˜è®¤: 128]: ").strip() or "128")
    configs['d_state'] = int(input("  çŠ¶æ€ç»´åº¦ [é»˜è®¤: 16]: ").strip() or "16")
    configs['max_length'] = int(input("  æœ€å¤§åºåˆ—é•¿åº¦ [é»˜è®¤: 64]: ").strip() or "64")
    
    print("\n2. è®­ç»ƒå‚æ•°")
    configs['batch_size'] = int(input("  æ‰¹æ¬¡å¤§å° [é»˜è®¤: 16]: ").strip() or "16")
    configs['num_epochs'] = int(input("  è®­ç»ƒè½®æ•° [é»˜è®¤: 30]: ").strip() or "30")
    configs['learning_rate'] = float(input("  å­¦ä¹ ç‡ [é»˜è®¤: 0.001]: ").strip() or "0.001")
    configs['train_split'] = float(input("  è®­ç»ƒé›†æ¯”ä¾‹ [é»˜è®¤: 0.8]: ").strip() or "0.8")
    
    print("\n3. è¾“å‡ºè®¾ç½®")
    configs['model_name'] = input("  æ¨¡å‹ä¿å­˜åç§° [é»˜è®¤: best_model]: ").strip() or "best_model"
    
    return configs


class DirectionDatasetBERT(Dataset):
    """ä½¿ç”¨BERT Tokenizerçš„æ•°æ®é›†"""
    
    def __init__(self, csv_path: str, max_length: int = 64, use_bert_tokenizer: bool = True):
        df = pd.read_csv(csv_path, encoding='utf-8')
        self.texts = df['text'].tolist()
        self.directions = df['direction'].tolist()
        
        self.max_length = max_length
        self.use_bert_tokenizer = use_bert_tokenizer and HAS_TRANSFORMERS
        
        self.direction_map = {
            'left': 0, 'right': 1, 'bidirectional': 2,
            'å·¦': 0, 'å³': 1, 'åŒå‘': 2,
            'L': 0, 'R': 1, 'B': 2
        }
        
        if self.use_bert_tokenizer:
            print("âœ“ ä½¿ç”¨BERT Tokenizer")
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
            self.vocab_size = self.tokenizer.vocab_size
        else:
            print("âœ“ ä½¿ç”¨å­—ç¬¦çº§Tokenizer")
            self._build_char_vocab()
        
        print(f"âœ“ è¯è¡¨å¤§å°: {self.vocab_size}")
    
    def _build_char_vocab(self):
        chars = set()
        for text in self.texts:
            chars.update(text)
        
        self.char_to_idx = {'<PAD>': 0, '<UNK>': 1}
        for idx, char in enumerate(sorted(chars)):
            self.char_to_idx[char] = idx + 2
        
        self.vocab_size = len(self.char_to_idx)
    
    def tokenize(self, text: str):
        if self.use_bert_tokenizer:
            encoded = self.tokenizer.encode(
                text,
                add_special_tokens=False,
                max_length=self.max_length,
                truncation=True,
                padding='max_length'
            )
            return torch.tensor(encoded, dtype=torch.long)
        else:
            tokens = [self.char_to_idx.get(char, 1) for char in text[:self.max_length]]
            if len(tokens) < self.max_length:
                tokens = tokens + [0] * (self.max_length - len(tokens))
            return torch.tensor(tokens, dtype=torch.long)
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        direction = self.directions[idx]
        
        input_ids = self.tokenize(text)
        direction_label = self.direction_map.get(direction, 0)
        
        return {
            'input_ids': input_ids,
            'direction': torch.tensor(direction_label, dtype=torch.long),
            'text': text
        }


def train_model(csv_path, config):
    """è®­ç»ƒæ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    print("\n[1/4] åŠ è½½æ•°æ®é›†...")
    dataset = DirectionDatasetBERT(
        csv_path=csv_path,
        max_length=config['max_length'],
        use_bert_tokenizer=True
    )
    
    train_size = int(config['train_split'] * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)
    
    print(f"âœ“ è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {val_size} æ ·æœ¬")
    
    print("\n[2/4] åˆå§‹åŒ–æ¨¡å‹...")
    model = MambaDirectionEstimator(
        vocab_size=dataset.vocab_size,
        d_model=config['d_model'],
        d_state=config['d_state'],
        dropout=0.1
    )
    model = model.to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {num_params:,} ({num_params/1e6:.2f}M)")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    criterion = nn.CrossEntropyLoss()
    
    best_val_acc = 0.0
    
    os.makedirs('checkpoints', exist_ok=True)
    
    print("\n[3/4] å¼€å§‹è®­ç»ƒ...")
    print("-"*70)
    
    for epoch in range(config['num_epochs']):
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['direction'].to(device)
            
            logits = model(input_ids)
            loss = criterion(logits, labels)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)
        
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100 * train_correct / train_total
        
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                labels = batch['direction'].to(device)
                
                logits = model(input_ids)
                loss = criterion(logits, labels)
                
                val_loss += loss.item()
                preds = torch.argmax(logits, dim=-1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100 * val_correct / val_total
        
        print(f'Epoch [{epoch+1}/{config["num_epochs"]}] '
              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% | '
              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            
            model_path = f'checkpoints/{config["model_name"]}.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'vocab_size': model.embedding.num_embeddings,
                'd_model': model.d_model,
                'd_state': model.d_state,
                'use_bert_tokenizer': dataset.use_bert_tokenizer,
                'config': config
            }, model_path)
            
            print(f'  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)')
        
        scheduler.step()
    
    print("\n[4/4] è®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"âœ… æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: checkpoints/{config['model_name']}.pth")
    print("="*70)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='äº¤äº’å¼è®­ç»ƒ Mamba Direction Estimator (æ”¯æŒå¤šæ•°æ®é›†)')
    parser.add_argument('-i', '--input', type=str, nargs='+', help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥å¤šä¸ªï¼‰')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, help='å­¦ä¹ ç‡')
    parser.add_argument('--model-name', type=str, help='æ¨¡å‹ä¿å­˜åç§°')
    parser.add_argument('--auto', action='store_true', help='ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œä¸äº¤äº’')
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸ“ Mamba Direction Estimator - äº¤äº’å¼è®­ç»ƒ (å¤šæ•°æ®é›†æ”¯æŒ)")
    print("="*70)
    
    if not HAS_TRANSFORMERS:
        print("\nâš ï¸  è­¦å‘Š: æœªå®‰è£…transformersåº“")
        print("å»ºè®®å®‰è£…: pip install transformers")
        use_bert = input("\næ˜¯å¦ç»§ç»­ä½¿ç”¨å­—ç¬¦çº§tokenizer? [y/N]: ").strip().lower()
        if use_bert != 'y':
            print("è®­ç»ƒå–æ¶ˆ")
            return
    
    # 1. é€‰æ‹©CSVæ–‡ä»¶
    if args.input:
        csv_paths = args.input
        for csv_path in csv_paths:
            if not os.path.exists(csv_path):
                print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")
                return
    else:
        csv_paths = select_csv_interactive()
    
    # 2. é¢„è§ˆæ•°æ®é›†
    if not preview_dataset(csv_paths):
        print("\nâŒ æ•°æ®é›†æ ¼å¼é”™è¯¯ï¼Œè®­ç»ƒå–æ¶ˆ")
        return
    
    # 3. åˆå¹¶æ•°æ®é›†ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
    if len(csv_paths) > 1:
        confirm = input("\næ˜¯å¦åˆå¹¶è¿™äº›æ•°æ®é›†è¿›è¡Œè®­ç»ƒ? [Y/n]: ").strip().lower()
        if confirm == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return
        
        csv_path = merge_datasets(csv_paths)
    else:
        csv_path = csv_paths[0]
        confirm = input("\næ˜¯å¦ä½¿ç”¨æ­¤æ•°æ®é›†è®­ç»ƒ? [Y/n]: ").strip().lower()
        if confirm == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return
    
    # 4. è·å–è®­ç»ƒé…ç½®
    if args.auto:
        print("\nä½¿ç”¨é»˜è®¤é…ç½®...")
        config = {
            'd_model': 128,
            'd_state': 16,
            'max_length': 64,
            'batch_size': args.batch_size or 16,
            'num_epochs': args.epochs or 30,
            'learning_rate': args.lr or 0.001,
            'train_split': 0.8,
            'model_name': args.model_name or 'best_model'
        }
    else:
        config = get_training_config()
        
        if args.batch_size:
            config['batch_size'] = args.batch_size
        if args.epochs:
            config['num_epochs'] = args.epochs
        if args.lr:
            config['learning_rate'] = args.lr
        if args.model_name:
            config['model_name'] = args.model_name
    
    print("\næœ€ç»ˆé…ç½®:")
    print("-"*70)
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("-"*70)
    
    if not args.auto:
        confirm = input("\nå¼€å§‹è®­ç»ƒ? [Y/n]: ").strip().lower()
        if confirm == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return
    
    # 5. å¼€å§‹è®­ç»ƒ
    try:
        train_model(csv_path, config)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()