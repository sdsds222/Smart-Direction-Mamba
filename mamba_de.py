# mamba_de.py (FIXED VERSION - ALL BUGS RESOLVED, INTERFACES UNCHANGED)
import os
import contextlib
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split, Subset
import pandas as pd
import numpy as np
from typing import Optional, Union

DEFAULT_BERT_NAME = os.environ.get('SDM_BERT_MODEL', 'xlm-roberta-base')
ENV_USE_FOCAL = os.environ.get('SDM_USE_FOCAL', '0') == '1'
ENV_FOCAL_GAMMA = float(os.environ.get('SDM_FOCAL_GAMMA', '2.0'))
ENV_EARLY_STOP = int(os.environ.get('SDM_EARLY_STOP_PATIENCE', '5'))
ENV_CLIP_MAX_NORM = float(os.environ.get('SDM_CLIP_MAX_NORM', '1.0'))

try:
    from transformers import AutoTokenizer, AutoModel, AutoConfig
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("âš ï¸ æœªå®‰è£… transformersï¼Œè¿è¡Œ: pip install transformers")

def _has_new_torch_amp():
    return hasattr(torch, "amp") and hasattr(torch.amp, "autocast")

_HAS_NEW_AMP = _has_new_torch_amp()

def _autocast_ctx(enabled=True):
    if not enabled:
        return contextlib.nullcontext()
    return torch.amp.autocast('cuda', enabled=True) if _HAS_NEW_AMP else torch.cuda.amp.autocast(enabled=True)

def _make_scaler(enabled=True):
    if not enabled:
        return None
    try:
        return torch.amp.GradScaler('cuda', enabled=True) if _HAS_NEW_AMP else torch.cuda.amp.GradScaler(enabled=True)
    except TypeError:
        return torch.cuda.amp.GradScaler(enabled=True)

class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, weight=None, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.weight = weight
        self.reduction = reduction

    def forward(self, logits, target):
        logpt = torch.log_softmax(logits, dim=-1)
        pt = torch.exp(logpt)
        logpt = logpt.gather(1, target.view(-1, 1)).squeeze(1)
        pt = pt.gather(1, target.view(-1, 1)).squeeze(1)
        if self.weight is not None:
            w = self.weight.to(logits.device).gather(0, target)
            loss = -w * (1 - pt).pow(self.gamma) * logpt
        else:
            loss = -(1 - pt).pow(self.gamma) * logpt
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

class SimplifiedMambaSSM(nn.Module):
    """
    ä¿®æ­£ç‚¹ï¼š
    1) å°† A æ”¹ä¸ºå¯¹è§’å‚æ•° A_diagï¼Œç¦»æ•£åŒ–ä½¿ç”¨ exp(Î” * A_diag) ä¸ h åšæŒ‰ç»´ç¼©æ”¾ï¼Œé¿å…é”™è¯¯çš„é€å…ƒç´ çŸ©é˜µæŒ‡æ•°ã€‚
    2) ä»ä¿æŒä¸åŸç±»å/æ¥å£ä¸€è‡´ã€‚
    """
    def __init__(self, d_model: int, d_state: int = 16):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state

        # ä½¿ç”¨å¯¹è§’ Aï¼šä¿è¯ e^{Î”A} = diag(e^{Î” a_i}) ç®€å•ç¨³å®š
        self.A_diag = nn.Parameter(torch.randn(d_state) * 0.01)

        self.delta_proj = nn.Linear(d_model, d_state, bias=False)
        self.B_proj = nn.Linear(d_model, d_state, bias=False)
        self.C_proj = nn.Linear(d_model, d_state, bias=False)
        self.out = nn.Linear(d_state, d_model, bias=False)
        self.D = nn.Parameter(torch.ones(d_model))
        self.input_gate = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model)
        )

        # ç¨³å®šæ€§ï¼šA å¯¹è§’ä¸ºè´Ÿ
        with torch.no_grad():
            self.A_diag.data = -self.A_diag.data.abs()

    def forward(self, x, h=None, mask: Optional[torch.Tensor] = None):
        B, T, _ = x.shape
        device = x.device
        if h is None:
            h = torch.zeros(B, self.d_state, device=device)
        if mask is None:
            mask = torch.ones(B, T, device=device)
        mask = mask.unsqueeze(-1)  # [B, T, 1]

        outs = []
        for t in range(T):
            x_t = x[:, t, :]
            m_t = mask[:, t, :]

            x_gated = torch.sigmoid(self.input_gate(x_t)) * x_t
            delta = F.softplus(self.delta_proj(x_gated))          # [B, d_state]
            B_t = self.B_proj(x_gated)                            # [B, d_state]
            C_t = self.C_proj(x_gated)                            # [B, d_state]

            # e^{Î” * A_diag}ï¼šæŒ‰ç»´ç¼©æ”¾
            A_bar = torch.exp(delta * self.A_diag)                # [B, d_state]
            h_candidate = torch.tanh(A_bar * h + B_t)             # [B, d_state]
            h = h_candidate * m_t + h * (1.0 - m_t)

            s_t = C_t * h                                         # [B, d_state]
            y_t = self.out(s_t) + self.D * x_t                    # [B, d_model]
            outs.append(y_t)

        return torch.stack(outs, dim=1), h  # [B, T, d_model], [B, d_state]

class MambaDirectionEstimator(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, d_state: int = 16, dropout: float = 0.5,
                 freeze_embedding: bool = False, bert_model_name: str = DEFAULT_BERT_NAME):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.freeze_embedding = freeze_embedding
        self.bert_model_name = bert_model_name
        self.dropout = dropout

        if HAS_TRANSFORMERS:
            print(f"ğŸš€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶æå– embedding: {bert_model_name}")
            try:
                cfg = AutoConfig.from_pretrained(bert_model_name)
                backbone = AutoModel.from_pretrained(bert_model_name, config=cfg, low_cpu_mem_usage=True)
            except Exception:
                backbone = AutoModel.from_pretrained(bert_model_name)

            bert_dim = backbone.config.hidden_size
            self.embedding = backbone.get_input_embeddings()
            tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
            self.pad_token_id = tokenizer.pad_token_id
            print(f"âœ“ Pad Token ID: {self.pad_token_id}")

            if freeze_embedding:
                print("â„ï¸ å†»ç»“ embedding æƒé‡")
                for p in self.embedding.parameters():
                    p.requires_grad = False
            else:
                print("ğŸ”¥ å¾®è°ƒ embedding æƒé‡")

            if bert_dim != d_model:
                self.projection = nn.Linear(bert_dim, d_model)
                print(f"ğŸ“ æ·»åŠ æŠ•å½±å±‚: {bert_dim} -> {d_model}")
            else:
                self.projection = None
            print("âœ“ ä½¿ç”¨é¢„è®­ç»ƒ embedding")
        else:
            self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
            self.projection = None
            self.pad_token_id = 0
            print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ– embeddingï¼ˆæœªå®‰è£… transformersï¼‰")

        self.forward_ssm = SimplifiedMambaSSM(d_model, d_state)
        self.backward_ssm = SimplifiedMambaSSM(d_model, d_state)

        self.state_analyzer = nn.Sequential(
            nn.Linear(d_state * 2, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, d_model)
        )

        self.pool_proj = nn.Linear(d_model * 2, d_model)

        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.LayerNorm(d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 3)
        )

    def _infer_mask(self, input_ids):
        pad_token_id = getattr(self, 'pad_token_id', 0)
        return (input_ids != pad_token_id).float()

    def forward(self, input_ids):
        # [B, T] -> [B, T, d_model]
        if self.projection is not None:
            x = self.projection(self.embedding(input_ids))
        else:
            x = self.embedding(input_ids)

        # æ¨æ–­æœ‰æ•ˆä½ mask
        mask = self._infer_mask(input_ids)             # [B, T]

        # å‰å‘ä¸åå‘ SSM
        forward_out, h_f = self.forward_ssm(x, mask=mask)  # [B, T, d], [B, s]
        x_reversed = torch.flip(x, dims=[1])
        mask_reversed = torch.flip(mask, dims=[1])
        backward_out, h_b = self.backward_ssm(x_reversed, mask=mask_reversed)
        backward_out = torch.flip(backward_out, dims=[1])  # å¯¹é½æ—¶é—´

        # æ‹¼æ¥åŒå‘è¾“å‡º
        combined = torch.cat([forward_out, backward_out], dim=-1)  # [B, T, 2d]

        # ---- å…³é”®ä¿®å¤ï¼šå¸¦ mask çš„æ± åŒ–ï¼Œé¿å… PAD ç¨€é‡Š ----
        mask_exp = mask.unsqueeze(-1)                               # [B, T, 1]
        combined_masked = combined * mask_exp                       # [B, T, 2d]
        denom = mask.sum(dim=1, keepdim=True).clamp_min(1e-6)       # [B, 1]
        pooled_seq = combined_masked.sum(dim=1) / denom             # [B, 2d]
        pooled = self.pool_proj(pooled_seq)                         # [B, d]
        # ---------------------------------------------------

        # èåˆç»ˆæ€
        final_state = torch.cat([h_f, h_b], dim=-1)                 # [B, 2s]
        enriched_state = self.state_analyzer(final_state)           # [B, d]

        logits = self.classifier(pooled + enriched_state)           # [B, 3]
        return logits

class DirectionDatasetBERT(Dataset):
    def __init__(self, csv_path, max_length=64, use_bert_tokenizer=True, bert_model_name=DEFAULT_BERT_NAME):
        df = pd.read_csv(csv_path)
        required_cols = {'text', 'direction'}
        if not required_cols.issubset(df.columns):
            raise ValueError(f"CSV å¿…é¡»åŒ…å«åˆ—: {required_cols}ï¼Œå®é™…: {df.columns.tolist()}")

        self.texts = df['text'].astype(str).tolist()
        raw_labels = df['direction'].tolist()

        label_map = {'left': 0, 'right': 1, 'bidirectional': 2}
        self.labels = []
        valid_indices = []
        for i, lbl in enumerate(raw_labels):
            lbl_low = str(lbl).strip().lower()
            if lbl_low in label_map:
                self.labels.append(label_map[lbl_low])
                valid_indices.append(i)

        self.texts = [self.texts[i] for i in valid_indices]
        if len(self.texts) == 0:
            raise ValueError("è¿‡æ»¤åæ— æœ‰æ•ˆæ ·æœ¬ã€‚")

        print(f"âœ“ æœ‰æ•ˆæ ·æœ¬æ•°: {len(self.texts)} (åŸå§‹ {len(df)} æ¡)")

        self.max_length = max_length
        self.use_bert_tokenizer = use_bert_tokenizer
        self.bert_model_name = bert_model_name

        if use_bert_tokenizer:
            if not HAS_TRANSFORMERS:
                raise RuntimeError("æœªå®‰è£… transformersï¼Œæ— æ³•ä½¿ç”¨ BERT tokenizer")
            self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
            self.vocab_size = self.tokenizer.vocab_size
            print(f"âœ“ ä½¿ç”¨ BERT Tokenizer: {bert_model_name}, vocab_size={self.vocab_size}")
        else:
            all_chars = set(''.join(self.texts))
            self.char_to_idx = {ch: i+1 for i, ch in enumerate(sorted(all_chars))}
            self.char_to_idx['<PAD>'] = 0
            self.vocab_size = len(self.char_to_idx)
            print(f"âœ“ ä½¿ç”¨å­—ç¬¦çº§ Tokenizer, vocab_size={self.vocab_size}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        if self.use_bert_tokenizer:
            enc = self.tokenizer(
                text,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt',
                add_special_tokens=True
            )
            input_ids = enc['input_ids'].squeeze(0)
        else:
            input_ids = [self.char_to_idx.get(ch, 0) for ch in text[:self.max_length]]
            pad_len = self.max_length - len(input_ids)
            if pad_len > 0:
                input_ids = input_ids + [0] * pad_len
            input_ids = torch.tensor(input_ids, dtype=torch.long)

        return {'input_ids': input_ids, 'direction': torch.tensor(label, dtype=torch.long)}

def _per_class_metrics(cm):
    num_classes = cm.shape[0]
    P, R, F1 = [], [], []
    for i in range(num_classes):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        p = tp / max(1, tp + fp)
        r = tp / max(1, tp + fn)
        f1 = 2 * p * r / max(1e-9, p + r)
        P.append(p)
        R.append(r)
        F1.append(f1)
    macro_p = sum(P) / num_classes
    macro_r = sum(R) / num_classes
    macro_f1 = sum(F1) / num_classes
    return P, R, F1, macro_p, macro_r, macro_f1

def train_mamba_de(model, train_loader, val_loader, num_epochs=30, lr=1e-3, device='cuda',
                   save_path='checkpoints/best_model.pth', use_focal=False, focal_gamma=2.0,
                   early_stop_patience=5, clip_max_norm=1.0, extra_meta=None):
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.to(device)

    # ç»Ÿè®¡ç±»æƒé‡
    all_labels = []
    for batch in train_loader:
        all_labels.extend(batch['direction'].tolist())
    label_counts = torch.bincount(torch.tensor(all_labels))
    total = label_counts.sum()
    class_weights = total / (label_counts.float() * 3 + 1e-6)
    class_weights = class_weights.to(device)
    print(f"ç±»åˆ«æƒé‡: {class_weights.tolist()}")

    # æŸå¤±å‡½æ•°
    if use_focal:
        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights)
        print(f"ä½¿ç”¨ Focal Loss (gamma={focal_gamma})")
    else:
        criterion = nn.CrossEntropyLoss(weight=class_weights)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    # AMP æ§åˆ¶ï¼šå°Šé‡è®¾å¤‡ä¸å¯é€‰ç¯å¢ƒå˜é‡ SDM_NO_AMPï¼ˆç”± main åœ¨ --no-amp æ—¶è®¾ç½®ï¼‰
    use_amp = (device == 'cuda') and (os.environ.get('SDM_NO_AMP', '0') != '1')
    scaler = _make_scaler(enabled=use_amp)

    best_val_acc = 0.0
    patience = early_stop_patience
    no_improve = 0
    direction_names = ['left', 'right', 'bidirectional']

    for epoch in range(num_epochs):
        model.train()
        tr_loss, tr_correct, tr_total = 0.0, 0, 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['direction'].to(device)
            optimizer.zero_grad()

            with _autocast_ctx(enabled=use_amp):
                logits = model(input_ids)
                loss = criterion(logits, labels)

            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_max_norm)
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_max_norm)
                optimizer.step()

            tr_loss += float(loss.item())
            preds = torch.argmax(logits.detach(), dim=-1)
            tr_correct += int((preds == labels).sum().item())
            tr_total += labels.size(0)

        avg_train_loss = tr_loss / max(1, len(train_loader))
        train_acc = 100.0 * tr_correct / max(1, tr_total)

        model.eval()
        va_loss, va_correct, va_total = 0.0, 0, 0
        cm = np.zeros((3, 3), dtype=int)

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                labels = batch['direction'].to(device)
                with _autocast_ctx(enabled=use_amp):
                    logits = model(input_ids)
                    loss = criterion(logits, labels)
                va_loss += float(loss.item())
                preds = torch.argmax(logits, dim=-1)
                va_correct += int((preds == labels).sum().item())
                va_total += labels.size(0)
                for t, p in zip(labels.cpu().numpy(), preds.cpu().numpy()):
                    cm[t][p] += 1

        avg_val_loss = va_loss / max(1, len(val_loader))
        val_acc = 100.0 * va_correct / max(1, va_total)
        P, R, F1, macro_p, macro_r, macro_f1 = _per_class_metrics(cm)

        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%')
        print(f'  éªŒè¯ - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%  | Macro P/R/F1: {macro_p:.3f}/{macro_r:.3f}/{macro_f1:.3f}')

        if (epoch + 1) % 5 == 0:
            print('\næ··æ·†çŸ©é˜µ:')
            header = 'å®é™…\\é¢„æµ‹'
            print(f'{header:12s}  ' + '  '.join([f'{n:12s}' for n in direction_names]))
            for i, row in enumerate(cm):
                print(f'{direction_names[i]:12s}  ' + '  '.join([f'{v:12d}' for v in row]))
            print('æŒ‰ç±» P/R/F1:')
            for i, name in enumerate(direction_names):
                print(f'  {name}: P={P[i]:.3f} R={R[i]:.3f} F1={F1[i]:.3f}')
            print()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            no_improve = 0

            ds_flag = True
            char_to_idx = None
            try:
                ds = train_loader.dataset
                real_ds = ds.dataset if isinstance(ds, Subset) else ds
                ds_flag = getattr(real_ds, 'use_bert_tokenizer', True)
                if not ds_flag:
                    char_to_idx = getattr(real_ds, 'char_to_idx', None)
            except Exception:
                pass

            save_dict = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'vocab_size': getattr(model.embedding, 'num_embeddings', None) or 0,
                'd_model': model.d_model,
                'd_state': model.d_state,
                'use_bert_tokenizer': ds_flag,
                'freeze_embedding': model.freeze_embedding,
                'bert_model_name': getattr(model, 'bert_model_name', DEFAULT_BERT_NAME),
                'pad_token_id': getattr(model, 'pad_token_id', 0),
                'class_weights': class_weights.detach().cpu().tolist(),
                'use_focal': use_focal,
                'focal_gamma': focal_gamma,
                'dropout': getattr(model, 'dropout', 0.5),
            }
            if char_to_idx is not None:
                save_dict['char_to_idx'] = char_to_idx
            if extra_meta:
                save_dict.update(extra_meta)

            torch.save(save_dict, save_path)
            print(f'  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ -> {save_path} (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)\n')
        else:
            no_improve += 1
            print(f'  â†³ éªŒè¯æœªæå‡ï¼ˆè¿ç»­ {no_improve}/{patience}ï¼‰\n')
            if no_improve >= patience:
                print(f'â¹ è§¦å‘ EarlyStoppingï¼ˆpatience={patience}ï¼‰ï¼Œæå‰ç»“æŸè®­ç»ƒã€‚')
                break

        scheduler.step()

    print(f'\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
    return model, device

def _build_cli():
    import argparse
    p = argparse.ArgumentParser(description="Mamba DE è®­ç»ƒï¼ˆå¤šè¯­ç§ + é¢„è®­ç»ƒEmbedding + AMP + æ—©åœï¼‰")
    p.add_argument('--csv', type=str, default='training_data.csv', help='è®­ç»ƒCSVè·¯å¾„')
    p.add_argument('--bert-model', type=str, default=DEFAULT_BERT_NAME, help='é¢„è®­ç»ƒæ¨¡å‹å/è·¯å¾„')
    p.add_argument('--d-model', type=int, default=128)
    p.add_argument('--d-state', type=int, default=16)
    p.add_argument('--dropout', type=float, default=0.5)
    p.add_argument('--freeze-embedding', action='store_true', help='å†»ç»“embedding')
    p.add_argument('--max-length', type=int, default=64)
    p.add_argument('--batch-size', type=int, default=16)
    p.add_argument('--epochs', type=int, default=30)
    p.add_argument('--lr', type=float, default=1e-3)
    p.add_argument('--train-split', type=float, default=0.8)
    p.add_argument('--use-focal', action='store_true')
    p.add_argument('--focal-gamma', type=float, default=2.0)
    p.add_argument('--early-stop', type=int, default=5, help='EarlyStopping patience')
    p.add_argument('--clip-max-norm', type=float, default=1.0)
    p.add_argument('--no-amp', action='store_true', help='ç¦ç”¨AMP')
    p.add_argument('--save-name', type=str, default='best_model_bert.pth', help='ä¿å­˜æ–‡ä»¶å')
    return p

def main():
    print("="*60)
    print("Mamba Direction Estimator - ALL BUGS FIXED")
    print("="*60)

    if not HAS_TRANSFORMERS:
        print("\nâŒ éœ€è¦å®‰è£… transformersï¼špip install transformers")
        return

    args = _build_cli().parse_args()

    # è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ï¼›ä¸æ”¹ CLI æ¥å£
    dev = 'cuda' if torch.cuda.is_available() else 'cpu'
    if args.no_amp:
        os.environ['SDM_NO_AMP'] = '1'  # train å†…éƒ¨è¯»å–ï¼Œä¿æŒå‡½æ•°ç­¾åä¸å˜

    dataset = DirectionDatasetBERT(
        csv_path=args.csv,
        max_length=args.max_length,
        use_bert_tokenizer=True,
        bert_model_name=args.bert_model
    )

    train_size = int(args.train_split * len(dataset))
    val_size = len(dataset) - train_size
    # å›ºå®šåˆ’åˆ†ç§å­ï¼Œå¢å¼ºå¯å¤ç°æ€§ï¼›ä¸æ”¹å‡½æ•°/å‚æ•°æ¥å£
    gen = torch.Generator().manual_seed(42)
    train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=gen)
    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False)
    print(f"âœ“ è®­ç»ƒé›†: {train_size} | éªŒè¯é›†: {val_size}")

    model = MambaDirectionEstimator(
        vocab_size=dataset.vocab_size,
        d_model=args.d_model,
        d_state=args.d_state,
        dropout=args.dropout,
        freeze_embedding=args.freeze_embedding,
        bert_model_name=args.bert_model
    )

    save_path = os.path.join('checkpoints', args.save_name)
    trained_model, device = train_mamba_de(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.epochs,
        lr=args.lr,
        device=dev,  # ä¸å†å†™æ­» 'cuda'
        save_path=save_path,
        use_focal=args.use_focal,
        focal_gamma=args.focal_gamma,
        early_stop_patience=args.early_stop,
        clip_max_norm=args.clip_max_norm,
        extra_meta={
            'max_length': args.max_length,
            'config': {
                'bert_model_name': args.bert_model,
                'max_length': args.max_length,
                'batch_size': args.batch_size,
                'num_epochs': args.epochs,
                'learning_rate': args.lr,
                'train_split': args.train_split,
                'd_model': args.d_model,
                'd_state': args.d_state,
                'dropout': args.dropout,
                'freeze_embedding': args.freeze_embedding
            }
        }
    )

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜: {save_path}")

if __name__ == '__main__':
    main()
