# train.py (Transformer Version)
import os
import glob
import argparse
import numpy as np
import pandas as pd
import torch
from typing import List
from torch.utils.data import DataLoader, Subset

from transformer_de import (
    DirectionDatasetBERT,
    TransformerDirectionEstimator,
    train_transformer_de,
)

try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False


def list_csv_files(directory='.') -> List[str]:
    return sorted(glob.glob(os.path.join(directory, '*.csv')))


def select_multiple_csv(csv_files: List[str]) -> List[str]:
    print("\n" + "="*70)
    print("ğŸ“š å¤šæ•°æ®é›†é€‰æ‹©")
    print("="*70)
    print("\næç¤ºï¼šè¾“å…¥æ•°å­—åºå·ï¼Œç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼ˆå¦‚: 1,2,3 æˆ– 1 2 3ï¼‰")
    print("è¾“å…¥ 'all' é€‰æ‹©å…¨éƒ¨\n")

    for i, file in enumerate(csv_files, 1):
        size_kb = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size_kb:.1f} KB, ~{total_rows} è¡Œ)")
        except Exception:
            print(f"  {i}. {os.path.basename(file):30s} ({size_kb:.1f} KB)")

    while True:
        selection = input(f"\nè¯·é€‰æ‹©è¦åˆå¹¶çš„æ•°æ®é›†: ").strip()
        if selection.lower() == 'all':
            return csv_files

        try:
            if ',' in selection:
                indices = [int(x.strip()) for x in selection.split(',') if x.strip()]
            else:
                indices = [int(x.strip()) for x in selection.split() if x.strip()]
            selected_files = []
            for idx in indices:
                if 1 <= idx <= len(csv_files):
                    selected_files.append(csv_files[idx-1])
                else:
                    print(f"âŒ åºå· {idx} æ— æ•ˆ")
                    break
            else:
                if selected_files:
                    print(f"\nâœ“ å·²é€‰æ‹© {len(selected_files)} ä¸ªæ•°æ®é›†:")
                    for f in selected_files:
                        print(f"  - {os.path.basename(f)}")
                    return selected_files
                print("âŒ æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶")
        except ValueError:
            print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—åºå·")


def select_csv_interactive() -> List[str]:
    print("\n" + "="*70)
    print("ğŸ“ é€‰æ‹©è®­ç»ƒæ•°æ®é›†")
    print("="*70)

    csv_files = list_csv_files()
    if not csv_files:
        print("âŒ å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        csv_path = input("\nè¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
        return [csv_path]

    print("\næ‰¾åˆ°ä»¥ä¸‹CSVæ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        size_kb = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size_kb:.1f} KB, ~{total_rows} è¡Œ)")
        except Exception:
            print(f"  {i}. {os.path.basename(file):30s} ({size_kb:.1f} KB)")

    print(f"  {len(csv_files)+1}. æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„")
    print(f"  {len(csv_files)+2}. é€‰æ‹©å¤šä¸ªæ•°æ®é›†")

    while True:
        choice = input(f"\nè¯·é€‰æ‹© [1-{len(csv_files)+2}]: ").strip()
        if not choice.isdigit():
            print("âŒ è¯·è¾“å…¥æ•°å­—")
            continue
        choice = int(choice)
        if 1 <= choice <= len(csv_files):
            return [csv_files[choice-1]]
        elif choice == len(csv_files) + 1:
            csv_path = input("è¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(csv_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
                continue
            return [csv_path]
        elif choice == len(csv_files) + 2:
            return select_multiple_csv(csv_files)
        else:
            print(f"âŒ è¯·è¾“å…¥ 1 - {len(csv_files)+2} ä¹‹é—´çš„æ•°å­—")


def merge_datasets(csv_paths: List[str]) -> str:
    print("\n" + "="*70)
    print("ğŸ”„ åˆå¹¶æ•°æ®é›†")
    print("="*70)

    all_dfs = []
    total_samples = 0

    for csv_path in csv_paths:
        print(f"\nè¯»å–: {os.path.basename(csv_path)}")
        df = pd.read_csv(csv_path, encoding='utf-8')
        if 'text' not in df.columns or 'direction' not in df.columns:
            print(f"  âš ï¸ è·³è¿‡: ç¼ºå°‘å¿…éœ€åˆ— (text, direction)")
            continue
        print(f"  âœ“ {len(df)} æ¡æ ·æœ¬")
        all_dfs.append(df)
        total_samples += len(df)

    if not all_dfs:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†å¯ä»¥åˆå¹¶")

    merged_df = pd.concat(all_dfs, ignore_index=True)
    print(f"\nâœ“ åˆå¹¶å®Œæˆ: æ€»è®¡ {total_samples} æ¡æ ·æœ¬")

    merged_path = 'merged_dataset.csv'
    merged_df.to_csv(merged_path, index=False, encoding='utf-8')
    print(f"âœ“ åˆå¹¶æ•°æ®é›†å·²ä¿å­˜è‡³: {merged_path}")
    return merged_path


def preview_dataset(csv_paths: List[str]) -> bool:
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é›†é¢„è§ˆ")
    print("="*70)

    if len(csv_paths) > 1:
        print(f"\nå…±é€‰æ‹© {len(csv_paths)} ä¸ªæ•°æ®é›†:")
        total_samples = 0
        all_directions = []

        for i, csv_path in enumerate(csv_paths, 1):
            df = pd.read_csv(csv_path, encoding='utf-8')
            print(f"\n{i}. {os.path.basename(csv_path)}")
            print(f"   æ ·æœ¬æ•°: {len(df)}")
            if 'direction' in df.columns:
                direction_counts = df['direction'].value_counts()
                print(f"   æ–¹å‘åˆ†å¸ƒ: " + ', '.join([f"{d}:{c}" for d, c in direction_counts.items()]))
                all_directions.extend(df['direction'].tolist())
            total_samples += len(df)

        print(f"\n{'='*70}")
        print(f"åˆå¹¶åç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")

        if all_directions:
            direction_counts = pd.Series(all_directions).value_counts()
            print(f"  æ–¹å‘åˆ†å¸ƒ:")
            for direction, count in direction_counts.items():
                pct = 100 * count / len(all_directions)
                print(f"    {direction}: {count} ({pct:.1f}%)")

            max_ratio = direction_counts.max() / max(1, direction_counts.min())
            if max_ratio > 3:
                print(f"  âš ï¸ æ•°æ®ä¸å¹³è¡¡ï¼ˆæœ€å¤§/æœ€å° = {max_ratio:.2f}ï¼‰")

        if total_samples < 50:
            print(f"  âš ï¸ æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼ˆå»ºè®®è‡³å°‘100æ¡ï¼‰")
        return True

    csv_path = csv_paths[0]
    df = pd.read_csv(csv_path, encoding='utf-8')

    print(f"\næ–‡ä»¶: {os.path.basename(csv_path)}")
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"åˆ—å: {list(df.columns)}")

    if 'text' not in df.columns:
        print("\nâš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° 'text' åˆ—")
        return False
    if 'direction' not in df.columns:
        print("\nâš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° 'direction' åˆ—")
        return False

    print("\næ–¹å‘åˆ†å¸ƒ:")
    direction_counts = df['direction'].value_counts()
    for direction, count in direction_counts.items():
        pct = 100 * count / len(df)
        print(f"  {direction}: {count} ({pct:.1f}%)")

    max_ratio = direction_counts.max() / max(1, direction_counts.min())
    if max_ratio > 3:
        print(f"\nâš ï¸ æ•°æ®ä¸å¹³è¡¡ï¼ˆæœ€å¤§/æœ€å° = {max_ratio:.2f}ï¼‰")
    if len(df) < 50:
        print(f"\nâš ï¸ æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼ˆå»ºè®®è‡³å°‘100æ¡ï¼‰")

    print("\nå‰3æ¡æ ·æœ¬:")
    print("-"*70)
    for i, row in df.head(3).iterrows():
        text = str(row['text'])[:50]
        direction = row['direction']
        print(f"{i+1}. [{direction}] {text}...")
    return True


def _labels_from_dataset(ds: DirectionDatasetBERT) -> np.ndarray:
    return np.array(ds.directions, dtype=np.int64)


def stratified_split_indices(labels: np.ndarray, train_ratio=0.8, seed=42):
    rs = np.random.RandomState(seed)
    train_idx, val_idx = [], []
    num_classes = int(labels.max()) + 1
    for c in range(num_classes):
        idx_c = np.where(labels == c)[0]
        rs.shuffle(idx_c)
        n_train = int(len(idx_c) * train_ratio)
        train_idx.extend(idx_c[:n_train].tolist())
        val_idx.extend(idx_c[n_train:].tolist())
    rs.shuffle(train_idx)
    rs.shuffle(val_idx)
    return train_idx, val_idx


def train_model(csv_path: str, config: dict):
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆTransformer + åˆ†å±‚åˆ’åˆ† + AMP + EarlyStoppingï¼‰")
    print("="*70)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")

    print("\n[1/4] åŠ è½½æ•°æ®é›†...")
    dataset = DirectionDatasetBERT(
        csv_path=csv_path,
        max_length=config['max_length'],
        use_bert_tokenizer=True,
        bert_model_name=config['bert_model']
    )

    labels = _labels_from_dataset(dataset)
    train_idx, val_idx = stratified_split_indices(labels, train_ratio=config['train_split'], seed=42)
    train_dataset = Subset(dataset, train_idx)
    val_dataset   = Subset(dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader   = DataLoader(val_dataset,   batch_size=config['batch_size'], shuffle=False)

    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ï¼ˆåˆ†å±‚åˆ‡åˆ†ï¼‰")

    print("\n[2/4] åˆå§‹åŒ–æ¨¡å‹...")
    model = TransformerDirectionEstimator(
        vocab_size=dataset.vocab_size,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dim_feedforward=config['dim_feedforward'],
        dropout=config.get('dropout', 0.5),
        freeze_embedding=config.get('freeze_embedding', False),
        bert_model_name=config['bert_model']
    ).to(device)

    num_params = sum(p.numel() for p in model.parameters())
    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ“ æ¨¡å‹æ€»å‚æ•°: {num_params:,} ({num_params/1e6:.2f}M)")
    print(f"âœ“ å¯è®­ç»ƒå‚æ•°: {num_trainable:,} ({num_trainable/1e6:.2f}M)")

    print("\n[3/4] å¼€å§‹è®­ç»ƒï¼ˆè°ƒç”¨ transformer_de.train_transformer_deï¼‰...")
    os.makedirs('checkpoints', exist_ok=True)
    save_path = os.path.join('checkpoints', f"{config['model_name']}.pth")

    trained_model, device = train_transformer_de(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=config['num_epochs'],
        lr=config['learning_rate'],
        device='cuda' if torch.cuda.is_available() else 'cpu',
        save_path=save_path,
        use_focal=config['use_focal'],
        focal_gamma=config['focal_gamma'],
        early_stop_patience=config['early_stop'],
        clip_max_norm=config['clip_max_norm'],
        extra_meta={
            'max_length': config['max_length'],
            'config': config
        }
    )

    print(f"\n[4/4] æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    print("="*70)
    print("âœ… è®­ç»ƒå®Œæˆï¼ˆè¯¦è§æ—¥å¿—ï¼šPer-class æŒ‡æ ‡ / æ··æ·†çŸ©é˜µ / EarlyStopping ç­‰ï¼‰")
    print("="*70)


def build_cli():
    p = argparse.ArgumentParser(
        description='äº¤äº’å¼è®­ç»ƒ Transformer Direction Estimator (å¤šæ•°æ®é›† + é¢„è®­ç»ƒEmbedding + åˆ†å±‚åˆ’åˆ†)'
    )
    p.add_argument('-i', '--input', type=str, nargs='+', help='è®­ç»ƒæ•°æ® CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¯å¤šä¸ªï¼‰')
    p.add_argument('--auto', action='store_true', help='ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œä¸äº¤äº’')
    p.add_argument('--bert-model', type=str, default='xlm-roberta-base', help='é¢„è®­ç»ƒæ¨¡å‹å/è·¯å¾„')
    p.add_argument('--d-model', type=int, default=128)
    p.add_argument('--nhead', type=int, default=4, help='æ³¨æ„åŠ›å¤´æ•°')
    p.add_argument('--num-layers', type=int, default=2, help='Transformerå±‚æ•°')
    p.add_argument('--dim-feedforward', type=int, default=512, help='FFNéšè—å±‚ç»´åº¦')
    p.add_argument('--dropout', type=float, default=0.5)
    p.add_argument('--freeze-embedding', action='store_true', help='å†»ç»“é¢„è®­ç»ƒ embeddingï¼ˆé»˜è®¤å¾®è°ƒï¼‰')
    p.add_argument('--max-length', type=int, default=64)
    p.add_argument('--train-split', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆåˆ†å±‚åˆ‡åˆ†ï¼‰')
    p.add_argument('--batch-size', type=int, default=16)
    p.add_argument('--epochs', type=int, default=30)
    p.add_argument('--lr', type=float, default=1e-3)
    p.add_argument('--use-focal', action='store_true', help='ä½¿ç”¨ FocalLossï¼ˆé»˜è®¤å…³é—­ï¼‰')
    p.add_argument('--focal-gamma', type=float, default=2.0)
    p.add_argument('--early-stop', type=int, default=5, help='EarlyStopping patience')
    p.add_argument('--clip-max-norm', type=float, default=1.0)
    p.add_argument('--model-name', type=str, default='best_model_transformer', help='ä¿å­˜åç§°ï¼ˆä¼šä¿å­˜åˆ° checkpoints/{name}.pthï¼‰')
    return p


def main():
    args = build_cli().parse_args()

    print("="*70)
    print("ğŸ“ Transformer Direction Estimator - äº¤äº’å¼è®­ç»ƒ (å¤šæ•°æ®é›† + é¢„è®­ç»ƒEmbedding + åˆ†å±‚åˆ’åˆ†)")
    print("="*70)

    if not HAS_TRANSFORMERS:
        print("\nâš ï¸ æœªå®‰è£… transformersï¼Œå°†æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨/Embeddingï¼špip install transformers")
        cont = input("æ˜¯å¦ç»§ç»­ï¼ˆå°†é€€å›å­—ç¬¦çº§Tokenizerï¼Œä¸å»ºè®®ï¼‰ï¼Ÿ[y/N]: ").strip().lower()
        if cont != 'y':
            print("è®­ç»ƒå–æ¶ˆ")
            return

    if args.input:
        csv_paths = args.input
        for pth in csv_paths:
            if not os.path.exists(pth):
                print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {pth}")
                return
    else:
        csv_paths = select_csv_interactive()

    if not preview_dataset(csv_paths):
        print("\nâŒ æ•°æ®é›†æ ¼å¼é”™è¯¯ï¼Œè®­ç»ƒå–æ¶ˆ")
        return

    if len(csv_paths) > 1:
        confirm = input("\næ˜¯å¦åˆå¹¶è¿™äº›æ•°æ®é›†è¿›è¡Œè®­ç»ƒ? [Y/n]: ").strip().lower()
        if confirm == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return
        csv_path = merge_datasets(csv_paths)
    else:
        csv_path = csv_paths[0]
        confirm = input("\næ˜¯å¦ä½¿ç”¨æ­¤æ•°æ®é›†è®­ç»ƒ? [Y/n]: ").strip().lower()
        if confirm == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return

    config = {
        'bert_model': args.bert_model,
        'd_model': args.d_model,
        'nhead': args.nhead,
        'num_layers': args.num_layers,
        'dim_feedforward': args.dim_feedforward,
        'max_length': args.max_length,
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': args.lr,
        'train_split': args.train_split,
        'dropout': args.dropout,
        'model_name': args.model_name,
        'freeze_embedding': args.freeze_embedding,
        'use_focal': args.use_focal,
        'focal_gamma': args.focal_gamma,
        'early_stop': args.early_stop,
        'clip_max_norm': args.clip_max_norm,
    }

    print("\næœ€ç»ˆé…ç½®:")
    print("-"*70)
    for k, v in config.items():
        print(f"  {k}: {v}")
    print("-"*70)

    if not args.auto:
        go = input("\nå¼€å§‹è®­ç»ƒ? [Y/n]: ").strip().lower()
        if go == 'n':
            print("è®­ç»ƒå–æ¶ˆ")
            return

    try:
        train_model(csv_path, config)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()