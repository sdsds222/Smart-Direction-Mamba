# predict.py (Transformer Version + ROUTING METRICS)
import os
import argparse
import glob
import time
import numpy as np
import pandas as pd
import torch
import contextlib
from typing import List

from transformer_de import TransformerDirectionEstimator

try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False


def list_model_files(directory='checkpoints') -> List[str]:
    if not os.path.exists(directory):
        return []
    model_files = glob.glob(os.path.join(directory, '*.pth'))
    return sorted(model_files, key=os.path.getmtime, reverse=True)


def select_model_interactive():
    print("\n" + "="*70)
    print("ğŸ¤– é€‰æ‹©æ¨¡å‹æ–‡ä»¶")
    print("="*70)

    model_files = list_model_files()
    if not model_files:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ (åœ¨checkpoints/ç›®å½•)")
        model_path = input("\nè¯·è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„: ").strip()
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {model_path}")
        return model_path

    print("\næ‰¾åˆ°ä»¥ä¸‹æ¨¡å‹:")
    for i, file in enumerate(model_files, 1):
        size = os.path.getsize(file) / (1024 * 1024)
        mtime = os.path.getmtime(file)
        time_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(mtime))
        try:
            checkpoint = torch.load(file, map_location='cpu')
            acc = checkpoint.get('val_acc', 0)
            freeze = checkpoint.get('freeze_embedding', False)
            embed_type = "å†»ç»“" if freeze else "å¾®è°ƒ"
            bert_name = checkpoint.get('bert_model_name') or checkpoint.get('config', {}).get('bert_model_name')
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} MB, Acc: {acc:.2f}%, {embed_type}, {bert_name}, {time_str})")
        except Exception:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} MB, {time_str})")

    print(f"  {len(model_files)+1}. æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„")

    while True:
        choice = input(f"\nè¯·é€‰æ‹© [1-{len(model_files)+1}]: ").strip()
        if not choice.isdigit():
            print("âŒ è¯·è¾“å…¥æ•°å­—")
            continue
        choice = int(choice)
        if 1 <= choice <= len(model_files):
            return model_files[choice-1]
        elif choice == len(model_files) + 1:
            model_path = input("è¯·è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(model_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {model_path}")
                continue
            return model_path
        else:
            print(f"âŒ è¯·è¾“å…¥1-{len(model_files)+1}ä¹‹é—´çš„æ•°å­—")


DIR_MAP_STR2IDX = {
    'left': 0, 'right': 1, 'bidirectional': 2,
    'å·¦': 0, 'å³': 1, 'åŒå‘': 2,
    'l': 0, 'r': 1, 'b': 2,
    'L': 0, 'R': 1, 'B': 2,
    '0': 0, '1': 1, '2': 2,
    0: 0, 1: 1, 2: 2,
}
DIR_IDX2NAME_CN = {0: 'æ­£å‘å› æœ(Aâ†’B)', 1: 'åå‘å› æœ(Aâ†B)', 2: 'åŒå‘å› æœ(Aâ†”B)'}

def _label_to_idx_strict(x):
    return DIR_MAP_STR2IDX.get(x, DIR_MAP_STR2IDX.get(str(x).strip(), None))

def _per_class_metrics(cm, eps=1e-12):
    P, R, F = [], [], []
    for k in range(cm.shape[0]):
        tp = cm[k, k]
        fp = cm[:, k].sum() - tp
        fn = cm[k, :].sum() - tp
        p = tp / (tp + fp + eps) if (tp + fp) > 0 else 0.0
        r = tp / (tp + fn + eps) if (tp + fn) > 0 else 0.0
        f1 = 2*p*r/(p+r+eps) if (p+r) > 0 else 0.0
        P.append(p); R.append(r); F.append(f1)
    return np.array(P), np.array(R), np.array(F), float(np.mean(P)), float(np.mean(R)), float(np.mean(F))


class Predictor:
    def __init__(self, model_path: str, max_length_override: int = None, use_amp: bool = True):
        self.raw_device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = self.raw_device
        self.use_amp = (use_amp and self.raw_device == 'cuda')

        ckpt = torch.load(model_path, map_location=self.device)

        bert_name = (ckpt.get('bert_model_name')
                     or ckpt.get('config', {}).get('bert_model_name')
                     or os.environ.get('SDM_BERT_MODEL', 'xlm-roberta-base'))

        self.use_bert_tokenizer = ckpt.get('use_bert_tokenizer', True)
        if self.use_bert_tokenizer:
            if not HAS_TRANSFORMERS:
                raise ImportError("éœ€è¦å®‰è£… transformers: pip install transformers")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(bert_name)
                self.pad_token_id = ckpt.get('pad_token_id', self.tokenizer.pad_token_id)
            except Exception as e:
                raise RuntimeError(f"æ— æ³•åŠ è½½åˆ†è¯å™¨ '{bert_name}': {e}")
        else:
            self.char_to_idx = ckpt.get('char_to_idx')
            if not self.char_to_idx:
                raise RuntimeError("Char æ¨¡å¼éœ€è¦ ckpt ä¸­çš„ 'char_to_idx'ã€‚")
            self.pad_token_id = 0

        freeze_embedding = ckpt.get('freeze_embedding', False)
        dropout = ckpt.get('dropout', ckpt.get('config', {}).get('dropout', 0.5))
        
        # Transformerå‚æ•°
        nhead = ckpt.get('nhead', ckpt.get('config', {}).get('nhead', 4))
        num_layers = ckpt.get('num_layers', ckpt.get('config', {}).get('num_layers', 2))
        dim_feedforward = ckpt.get('dim_feedforward', ckpt.get('config', {}).get('dim_feedforward', 512))
        
        # æ˜¾å¼Attentionåˆ†æå‚æ•°ï¼ˆå…¼å®¹æ—§æ¨¡å‹ï¼‰
        use_explicit_attention = ckpt.get('use_explicit_attention', ckpt.get('config', {}).get('use_explicit_attention', True))
        explicit_weight = ckpt.get('explicit_weight', ckpt.get('config', {}).get('explicit_weight', 0.3))

        self.model = TransformerDirectionEstimator(
            vocab_size=ckpt.get('vocab_size', 0),
            d_model=ckpt['d_model'],
            nhead=nhead,
            num_layers=num_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            freeze_embedding=freeze_embedding,
            bert_model_name=bert_name,
            use_explicit_attention=use_explicit_attention,
            explicit_weight=explicit_weight
        )
        self.model.load_state_dict(ckpt['model_state_dict'], strict=True)
        self.model = self.model.to(self.device)
        self.model.eval()

        self.max_length = (max_length_override
                           if isinstance(max_length_override, int) and max_length_override > 0
                           else ckpt.get('max_length', ckpt.get('config', {}).get('max_length', 64)))

        try:
            vocab_model = getattr(self.model.embedding, 'num_embeddings', None)
            vocab_tok = getattr(self.tokenizer, 'vocab_size', None) if self.use_bert_tokenizer else len(self.char_to_idx)
            if isinstance(vocab_model, int) and isinstance(vocab_tok, int) and vocab_model != vocab_tok:
                print(f"âš ï¸ åˆ†è¯å™¨è¯è¡¨({vocab_tok})ä¸æ¨¡å‹embeddingè¯è¡¨({vocab_model})ä¸ä¸€è‡´ã€‚æ¨ç†å¯èƒ½å—å½±å“ï¼Œè¯·ç¡®è®¤ tokenizer ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚")
        except Exception:
            pass

        self.direction_map = {0: 'left', 1: 'right', 2: 'bidirectional'}
        self.direction_names = DIR_IDX2NAME_CN

        amp_str = "å¼€å¯" if self.use_amp else "å…³é—­"
        tokenizer_type = "AutoTokenizer" if self.use_bert_tokenizer else "Char"
        embed_type = "å†»ç»“" if freeze_embedding else "å¾®è°ƒ"
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½ (Tokenizer: {tokenizer_type}, Embedding: {embed_type}, è®¾å¤‡: {self.device}, AMP: {amp_str})")
        print(f"âœ“ ä½¿ç”¨ tokenizer: {bert_name}")
        print(f"âœ“ max_length: {self.max_length}")
        print(f"âœ“ pad_token_id: {self.pad_token_id}")
        if 'val_acc' in ckpt:
            print(f"âœ“ è®­ç»ƒæ—¶éªŒè¯å‡†ç¡®ç‡: {ckpt['val_acc']:.2f}%")

    def _amp_ctx(self):
        if self.use_amp and self.device == 'cuda':
            return torch.amp.autocast('cuda', dtype=torch.float16, enabled=True)
        return contextlib.nullcontext()

    def _tokenize_one(self, text: str) -> torch.Tensor:
        if self.use_bert_tokenizer:
            enc = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                truncation=True,
                padding='max_length',
                return_tensors='pt'
            )
            return enc['input_ids'].squeeze(0).to(torch.long)
        else:
            tokens = [self.char_to_idx.get(ch, 1) for ch in text[:self.max_length]]
            if len(tokens) < self.max_length:
                tokens += [0] * (self.max_length - len(tokens))
            return torch.tensor(tokens, dtype=torch.long)

    def predict_one(self, text: str):
        input_ids = self._tokenize_one(text).unsqueeze(0).to(self.device)
        with torch.no_grad(), self._amp_ctx():
            logits = self.model(input_ids)
        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
        pred_idx = int(probs.argmax())
        confidence = float(probs[pred_idx])

        return {
            'text': text,
            'pred_label': self.direction_map[pred_idx],
            'pred_label_cn': self.direction_names[pred_idx],
            'confidence': confidence,
            'prob_left': float(probs[0]),
            'prob_right': float(probs[1]),
            'prob_bidirectional': float(probs[2]),
        }

    def predict_csv(self, input_csv: str, output_csv: str, text_col: str = 'text',
                    label_col: str = 'direction', batch_size: int = 64, verbose: bool = True):
        if not os.path.exists(input_csv):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_csv}")

        df = pd.read_csv(input_csv, encoding='utf-8')
        if text_col not in df.columns:
            raise ValueError(f"CSVç¼ºå°‘'{text_col}'åˆ—")

        has_label_col = (label_col in df.columns)

        if verbose:
            print(f"\nå¤„ç†CSV: {input_csv}")
            print(f"  å…± {len(df)} æ¡æ ·æœ¬")
            if has_label_col:
                print(f"  åŒ…å«æ ‡ç­¾åˆ— '{label_col}'ï¼Œå°†è®¡ç®—è¯„ä¼°æŒ‡æ ‡")

        texts = df[text_col].astype(str).tolist()
        input_ids_list = [self._tokenize_one(t) for t in texts]
        batch_data = torch.stack(input_ids_list).to(self.device)

        all_probs = []
        total_batches = (len(batch_data) + batch_size - 1) // batch_size
        iterator = range(total_batches)
        if verbose and HAS_TQDM:
            iterator = tqdm(iterator, desc="æ‰¹é‡é¢„æµ‹")

        with torch.no_grad():
            for i in iterator:
                start = i * batch_size
                end = min(start + batch_size, len(batch_data))
                batch = batch_data[start:end]
                with self._amp_ctx():
                    logits = self.model(batch)
                probs_batch = torch.softmax(logits, dim=-1).cpu().numpy()
                all_probs.append(probs_batch)

        all_probs = np.vstack(all_probs)
        preds_idx = np.argmax(all_probs, axis=1)
        confidences = all_probs[np.arange(len(all_probs)), preds_idx]

        df['pred_direction'] = [self.direction_map[int(i)] for i in preds_idx]
        df['pred_direction_cn'] = [self.direction_names[int(i)] for i in preds_idx]
        df['confidence'] = confidences
        df['prob_left'] = all_probs[:, 0]
        df['prob_right'] = all_probs[:, 1]
        df['prob_bidirectional'] = all_probs[:, 2]

        if has_label_col:
            labels_raw = df[label_col].tolist()
            labels_idx = [_label_to_idx_strict(lbl) for lbl in labels_raw]
            cm = np.zeros((3, 3), dtype=np.int64)
            valid_eval_count = 0
            for pred, gold in zip(preds_idx, labels_idx):
                if gold is not None and gold in (0, 1, 2):
                    cm[gold, pred] += 1
                    valid_eval_count += 1

        # è·¯ç”±æŒ‡æ ‡
        p_left = all_probs[:, 0]
        p_right = all_probs[:, 1]
        p_bi = all_probs[:, 2]
        route_final = []
        p_nonleft_list = []
        for j in range(len(preds_idx)):
            is_nonleft = 1 if preds_idx[j] in (1, 2) else 0
            route_final.append('bidir' if is_nonleft else 'forward')
            p_nonleft_list.append(float(p_right[j] + p_bi[j]))

        df['route_final'] = route_final
        df['p_nonleft'] = p_nonleft_list

        if has_label_col:
            gold_route_bin = []
            for lab in labels_idx:
                if lab in (0, 1, 2):
                    gold_route_bin.append(0 if lab == 0 else 1)
                else:
                    gold_route_bin.append(None)

            pred_route_bin = [0 if idx == 0 else 1 for idx in preds_idx]

            cm2 = np.zeros((2, 2), dtype=np.int64)
            valid2 = 0
            for g, p in zip(gold_route_bin, pred_route_bin):
                if g is None:
                    continue
                cm2[g, p] += 1
                valid2 += 1

        os.makedirs(os.path.dirname(os.path.abspath(output_csv)) or '.', exist_ok=True)
        df.to_csv(output_csv, index=False, encoding='utf-8')
        print(f"âœ“ å·²ä¿å­˜é¢„æµ‹ç»“æœ: {output_csv} ï¼ˆ{len(df)} æ¡ï¼‰")

        if has_label_col and valid_eval_count > 0:
            total = cm.sum()
            acc = np.trace(cm) / max(1, total)
            P, R, F1, mP, mR, mF1 = _per_class_metrics(cm)

            print("\nè¯„ä¼°æŒ‡æ ‡ï¼ˆè·³è¿‡æœªçŸ¥æ ‡ç­¾æ ·æœ¬ï¼‰")
            print("-"*70)
            print(f"æœ‰æ•ˆè¯„æµ‹æ ·æœ¬: {valid_eval_count} / {len(df)}")
            print(f"Overall Acc: {acc*100:.2f}%  |  Macro P/R/F1: {mP:.3f}/{mR:.3f}/{mF1:.3f}")
            print("\næ··æ·†çŸ©é˜µ:")
            header = 'å®é™…\\é¢„æµ‹'
            col_names = [DIR_IDX2NAME_CN[i] for i in range(3)]
            print(f'{header:12s}  ' + '  '.join([f'{name:12s}' for name in col_names]))
            for i in range(3):
                row = '  '.join([f'{int(v):12d}' for v in cm[i]])
                print(f'{DIR_IDX2NAME_CN[i]:12s}  {row}')
            print("\næŒ‰ç±» P/R/F1:")
            for i, name in enumerate(col_names):
                print(f"  {name}: P={P[i]:.3f} R={R[i]:.3f} F1={F1[i]:.3f}")
            print("-"*70)
        elif has_label_col:
            print("\nâš ï¸ æ²¡æœ‰ä»»ä½•åˆæ³•æ ‡ç­¾æ ·æœ¬ï¼ˆ0/1/2 æˆ– left/right/bidirectional/ä¸­æ–‡/LRBï¼‰ï¼Œè·³è¿‡ä¸‰åˆ†ç±»æŒ‡æ ‡è®¡ç®—ã€‚")

        if has_label_col:
            try:
                if valid2 > 0:
                    total2 = cm2.sum()
                    acc2 = np.trace(cm2) / max(1, total2)
                    P2, R2, F12, mP2, mR2, mF12 = _per_class_metrics(cm2)

                    print("\n[è·¯ç”±å±‚ (forward vs bidir) æŒ‡æ ‡]")
                    print("-"*70)
                    print(f"æœ‰æ•ˆè¯„æµ‹æ ·æœ¬(äºŒåˆ†ç±»): {valid2} / {len(df)}")
                    print(f"Routing Acc: {acc2*100:.2f}%  |  Macro P/R/F1: {mP2:.3f}/{mR2:.3f}/{mF12:.3f}")
                    print("\näºŒåˆ†ç±»æ··æ·†çŸ©é˜µ (è¡Œ=çœŸå® gold, åˆ—=é¢„æµ‹ pred)")
                    print("            pred: forward    bidir")
                    print(f"gold: forward    {int(cm2[0,0]):10d} {int(cm2[0,1]):10d}")
                    print(f"gold: bidir      {int(cm2[1,0]):10d} {int(cm2[1,1]):10d}")
                    print("\næŒ‰ç±» P/R/F1:")
                    print(f"  forward: P={P2[0]:.3f} R={R2[0]:.3f} F1={F12[0]:.3f}")
                    print(f"  bidir  : P={P2[1]:.3f} R={R2[1]:.3f} F1={F12[1]:.3f}")
                    print("-"*70)
                else:
                    print("\nâš ï¸ äºŒåˆ†ç±»è¯„ä¼°ï¼šæ²¡æœ‰ä»»ä½•åˆæ³•è·¯ç”±é‡‘æ ‡ï¼Œè·³è¿‡ã€‚")
            except NameError:
                pass

        return df, (cm if (has_label_col and valid_eval_count > 0) else None)


def interactive_mode(predictor: Predictor):
    print("\n" + "="*70)
    print("ğŸ¯ äº¤äº’å¼é¢„æµ‹æ¨¡å¼")
    print("="*70)
    print("è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ 'quit' / 'q' é€€å‡º")
    print("-"*70)

    while True:
        text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
        if text.lower() in ['quit', 'q', 'exit', 'é€€å‡º']:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        if not text:
            print("âš ï¸ æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue

        try:
            r = predictor.predict_one(text)
            print("\n" + "="*70)
            print("ğŸ“Š é¢„æµ‹ç»“æœ")
            print("="*70)
            print(f"æ–‡æœ¬: {text}")
            print(f"\nğŸ¯ é¢„æµ‹æ–¹å‘: {r['pred_label_cn']} ({r['pred_label']})")
            print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {r['confidence']:.2%}")
            print("\nè¯¦ç»†æ¦‚ç‡åˆ†å¸ƒï¼š")
            bar = lambda x: 'â–ˆ' * int(x * 30)
            print(f"  æ­£å‘å› æœ(Aâ†’B):   {r['prob_left']:.2%} {bar(r['prob_left'])}")
            print(f"  åå‘å› æœ(Aâ†B):   {r['prob_right']:.2%} {bar(r['prob_right'])}")
            print(f"  åŒå‘å› æœ(Aâ†”B):   {r['prob_bidirectional']:.2%} {bar(r['prob_bidirectional'])}")

            p_nonleft = r['prob_right'] + r['prob_bidirectional']
            final_route = 'bidir' if p_nonleft >= 0.5 else 'forward'
            print(f"\nğŸ§­ æœ€ç»ˆè·¯ç”±æ–¹å‘(ç­–ç•¥)ï¼š{final_route}  (p_nonleft={p_nonleft:.2%})")
            print("="*70)
        except Exception as e:
            print(f"\nâŒ é¢„æµ‹å‡ºé”™: {e}")


def main():
    parser = argparse.ArgumentParser(description='é¢„æµ‹è„šæœ¬ - æ”¯æŒæ¨¡å‹é€‰æ‹©ã€CSVæ‰¹é‡é¢„æµ‹å’Œäº¤äº’å¼é¢„æµ‹ï¼ˆTransformerç‰ˆæœ¬ï¼‰')
    parser.add_argument('-m', '--model', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-i', '--input', type=str, nargs='+', help='è¾“å…¥CSVæ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰')
    parser.add_argument('-o', '--output-dir', type=str, default='.', help='è¾“å‡ºç›®å½•')
    parser.add_argument('-t', '--text-col', type=str, default='text', help='æ–‡æœ¬åˆ—å')
    parser.add_argument('--label-col', type=str, default='direction', help='æ ‡ç­¾åˆ—åï¼ˆè‹¥å­˜åœ¨åˆ™è®¡ç®—æŒ‡æ ‡ï¼‰')
    parser.add_argument('--batch-size', type=int, default=64, help='æ¨ç†æ‰¹å¤§å°')
    parser.add_argument('--max-length', type=int, default=None, help='å¯è¦†ç›– ckpt çš„ max_length')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼é¢„æµ‹æ¨¡å¼')
    parser.add_argument('--no-amp', action='store_true', help='å…³é—­ AMPï¼ˆé»˜è®¤å¼€å¯ï¼ŒCUDA å¯ç”¨æ—¶ç”Ÿæ•ˆï¼‰')

    args = parser.parse_args()

    print("="*70)
    print("ğŸ¯ Transformer Direction Estimator - é¢„æµ‹å·¥å…·")
    print("="*70)

    if args.model:
        model_path = args.model
        if not os.path.exists(model_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            return
    else:
        model_path = select_model_interactive()

    try:
        predictor = Predictor(
            model_path=model_path,
            max_length_override=args.max_length,
            use_amp=(not args.no_amp)
        )
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    if not args.interactive:
        if args.input:
            csv_paths = args.input
            for p in csv_paths:
                if not os.path.exists(p):
                    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {p}")
                    return
        else:
            csv_paths = []

        if csv_paths:
            os.makedirs(args.output_dir, exist_ok=True)
            for csv_path in csv_paths:
                out_name = os.path.basename(csv_path).replace('.csv', '_predicted.csv')
                out_path = os.path.join(args.output_dir, out_name)
                try:
                    predictor.predict_csv(
                        input_csv=csv_path,
                        output_csv=out_path,
                        text_col=args.text_col,
                        label_col=args.label_col,
                        batch_size=args.batch_size,
                        verbose=True
                    )
                except Exception as e:
                    print(f"\nâŒ å¤„ç† {csv_path} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()

    if args.interactive or not args.input:
        interactive_mode(predictor)


if __name__ == '__main__':
    main()