# train_add.py (Transformer Version)
import os
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader, Subset

from transformer_de import (
    DirectionDatasetBERT,
    TransformerDirectionEstimator,
    train_transformer_de,
)

try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False


def _labels_from_dataset(ds: DirectionDatasetBERT) -> np.ndarray:
    return np.array(ds.directions, dtype=np.int64)


def stratified_split_indices(labels: np.ndarray, train_ratio=0.8, seed=42):
    rs = np.random.RandomState(seed)
    train_idx, val_idx = [], []
    num_classes = int(labels.max()) + 1
    for c in range(num_classes):
        idx_c = np.where(labels == c)[0]
        rs.shuffle(idx_c)
        n_train = int(len(idx_c) * train_ratio)
        train_idx.extend(idx_c[:n_train].tolist())
        val_idx.extend(idx_c[n_train:].tolist())
    rs.shuffle(train_idx)
    rs.shuffle(val_idx)
    return train_idx, val_idx


def fine_tune(base_model_path: str, data_csv: str, cfg: dict):
    print("\n" + "="*70)
    print("ğŸ”„ å¢é‡è®­ç»ƒ (Fine-tuning) - Transformer / åˆ†å±‚åˆ’åˆ† / AMP / EarlyStopping")
    print("="*70)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")

    print(f"\n[1/5] åŠ è½½åŸºç¡€æ¨¡å‹: {os.path.basename(base_model_path)}")
    base_ckpt = torch.load(base_model_path, map_location='cpu')

    bert_in_ckpt = base_ckpt.get('bert_model_name')
    if not bert_in_ckpt:
        cfg_in_ckpt = base_ckpt.get('config', {}) if isinstance(base_ckpt.get('config', {}), dict) else {}
        for k in ['bert_model_name', 'bert_name', 'pretrained_name', 'tokenizer_name', 'hf_model_name']:
            if k in cfg_in_ckpt:
                bert_in_ckpt = cfg_in_ckpt[k]
                break
    bert_model = cfg.get('bert_model') or bert_in_ckpt or 'xlm-roberta-base'
    if cfg.get('bert_model') and cfg['bert_model'] != bert_in_ckpt:
        print(f"â„¹ï¸ ä½¿ç”¨å‘½ä»¤è¡Œè¦†ç›–é¢„è®­ç»ƒæ¨¡å‹: {bert_in_ckpt} â†’ {cfg['bert_model']}")
    print(f"âœ“ é¢„è®­ç»ƒæ¨¡å‹: {bert_model}")

    freeze_from_ckpt = bool(base_ckpt.get('freeze_embedding', False))
    freeze_embedding = cfg.get('freeze_embedding')
    if freeze_embedding is None:
        freeze_embedding = freeze_from_ckpt
    else:
        if freeze_embedding != freeze_from_ckpt:
            print(f"â„¹ï¸ å†»ç»“ç­–ç•¥è¦†ç›–: {freeze_from_ckpt} â†’ {freeze_embedding}")
    print(f"âœ“ å†»ç»“ embedding: {freeze_embedding}")

    dropout = cfg.get('dropout', None)
    if dropout is None:
        dropout = base_ckpt.get('dropout', base_ckpt.get('config', {}).get('dropout', 0.5))

    pad_token_id = base_ckpt.get('pad_token_id', 0)

    # Transformerå‚æ•°
    nhead = base_ckpt.get('nhead', base_ckpt.get('config', {}).get('nhead', 4))
    num_layers = base_ckpt.get('num_layers', base_ckpt.get('config', {}).get('num_layers', 2))
    dim_feedforward = base_ckpt.get('dim_feedforward', base_ckpt.get('config', {}).get('dim_feedforward', 512))
    
    # æ˜¾å¼Attentionåˆ†æå‚æ•°ï¼ˆå…¼å®¹æ—§æ¨¡å‹ï¼‰
    use_explicit_attention = base_ckpt.get('use_explicit_attention', base_ckpt.get('config', {}).get('use_explicit_attention', True))
    explicit_weight = base_ckpt.get('explicit_weight', base_ckpt.get('config', {}).get('explicit_weight', 0.3))

    model = TransformerDirectionEstimator(
        vocab_size=base_ckpt.get('vocab_size', 0),
        d_model=base_ckpt['d_model'],
        nhead=nhead,
        num_layers=num_layers,
        dim_feedforward=dim_feedforward,
        dropout=dropout,
        freeze_embedding=freeze_embedding,
        bert_model_name=bert_model,
        use_explicit_attention=use_explicit_attention,
        explicit_weight=explicit_weight
    )
    model.load_state_dict(base_ckpt['model_state_dict'])
    model = model.to(device)
    
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"âœ“ å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    print(f"\n[2/5] åŠ è½½æ–°æ•°æ®å¹¶åˆ†å±‚åˆ’åˆ†...")
    ds = DirectionDatasetBERT(
        csv_path=data_csv,
        max_length=cfg['max_length'],
        use_bert_tokenizer=True,
        bert_model_name=bert_model,
    )
    
    try:
        vocab_model = getattr(model.embedding, 'num_embeddings', None)
        vocab_tok = getattr(ds, 'vocab_size', None)
        if isinstance(vocab_model, int) and isinstance(vocab_tok, int) and vocab_model != vocab_tok:
            print(f"âš ï¸ åˆ†è¯å™¨è¯è¡¨({vocab_tok})ä¸æ¨¡å‹embeddingè¯è¡¨({vocab_model})ä¸ä¸€è‡´ã€‚")
            print(f"   è¯·ç¡®è®¤ tokenizer ä¸åŸºç¡€æ¨¡å‹ä¸€è‡´ï¼ˆ--bert-modelï¼‰ã€‚")
    except Exception:
        pass

    labels = _labels_from_dataset(ds)
    if len(ds) < 10:
        print(f"âš ï¸ æ–°æ•°æ®é‡è¿‡å°‘ ({len(ds)} æ¡)ï¼Œå°†ä½¿ç”¨å…¨é‡è®­ç»ƒï¼Œä¸åˆ’åˆ†éªŒè¯é›†ã€‚")
        train_idx = np.arange(len(ds)).tolist()
        val_idx = []
    else:
        train_idx, val_idx = stratified_split_indices(labels, train_ratio=cfg['train_split'], seed=cfg['seed'])

    train_ds = Subset(ds, train_idx)
    val_ds   = Subset(ds, val_idx) if len(val_idx) > 0 else None

    train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False) if val_ds is not None else None

    print(f"âœ“ æ–°è®­ç»ƒé›†: {len(train_ds)} æ ·æœ¬")
    print(f"âœ“ æ–°éªŒè¯é›†: {len(val_ds) if val_ds is not None else 0} æ ·æœ¬ï¼ˆåˆ†å±‚åˆ‡åˆ†ï¼‰")

    base_lr = cfg['learning_rate']
    lr = base_lr * cfg['ft_scale']
    if cfg['ft_scale'] != 1.0:
        print(f"\n[3/5] å¾®è°ƒå­¦ä¹ ç‡ç¼©æ”¾: {base_lr} Ã— {cfg['ft_scale']} â†’ {lr}")
    else:
        print(f"\n[3/5] å­¦ä¹ ç‡: {lr}")

    print(f"\n[4/5] å¼€å§‹å¢é‡è®­ç»ƒï¼ˆå¤ç”¨ç»Ÿä¸€è®­ç»ƒç®¡çº¿ï¼‰...")
    os.makedirs('checkpoints', exist_ok=True)
    save_path = os.path.join('checkpoints', f"{cfg['model_name']}.pth")

    extra_meta = {
        'base_model': base_model_path,
        'base_val_acc': float(base_ckpt.get('val_acc', 0.0)),
        'fine_tuned': True,
        'max_length': cfg['max_length'],
        'config': cfg,
        'bert_model_name': bert_model,
        'pad_token_id': pad_token_id,
    }

    trained_model, _ = train_transformer_de(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader if val_loader is not None else train_loader,
        num_epochs=cfg['num_epochs'],
        lr=lr,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        save_path=save_path,
        use_focal=cfg['use_focal'],
        focal_gamma=cfg['focal_gamma'],
        early_stop_patience=cfg['early_stop'],
        clip_max_norm=cfg['clip_max_norm'],
        extra_meta=extra_meta
    )

    print(f"\n[5/5] å¢é‡è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    print("="*70)
    print("æç¤ºï¼šé¢„æµ‹æ—¶ç›´æ¥ä½¿ç”¨è¯¥è·¯å¾„ï¼Œä¾‹å¦‚ï¼š")
    print(f"  python predict.py -m {save_path} -i your_eval.csv --batch-size 64")
    print("="*70)


def build_cli():
    p = argparse.ArgumentParser(description='å¢é‡è®­ç»ƒ - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼ˆTransformerç‰ˆæœ¬ï¼‰')
    p.add_argument('-m', '--model', type=str, required=True, help='åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆ.pthï¼‰')
    p.add_argument('-i', '--input', type=str, required=True, help='æ–°è®­ç»ƒæ•°æ® CSV')
    p.add_argument('--bert-model', type=str, default=None, help='è¦†ç›–åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒåç§°/è·¯å¾„ï¼ˆé»˜è®¤æ²¿ç”¨ ckpt é‡Œè®°å½•çš„ï¼‰')
    p.add_argument('--freeze-embedding', dest='freeze_embedding', action='store_true', help='å¼ºåˆ¶å†»ç»“é¢„è®­ç»ƒ embeddingï¼ˆé»˜è®¤æ²¿ç”¨ ckptï¼‰')
    p.add_argument('--no-freeze-embedding', dest='freeze_embedding', action='store_false', help='å¼ºåˆ¶å¾®è°ƒé¢„è®­ç»ƒ embeddingï¼ˆé»˜è®¤æ²¿ç”¨ ckptï¼‰')
    p.set_defaults(freeze_embedding=None)
    p.add_argument('--max-length', type=int, default=64, help='åºåˆ—æœ€å¤§é•¿åº¦')
    p.add_argument('--train-split', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆåˆ†å±‚åˆ‡åˆ†ï¼‰')
    p.add_argument('--batch-size', type=int, default=16)
    p.add_argument('--epochs', type=int, default=15)
    p.add_argument('--lr', type=float, default=1e-3, help='åŸºç¡€å­¦ä¹ ç‡ï¼ˆå°†ä¹˜ä»¥ --ft-scale ç”¨äºå¾®è°ƒï¼‰')
    p.add_argument('--ft-scale', type=float, default=0.1, help='å¾®è°ƒå­¦ä¹ ç‡ç¼©æ”¾ç³»æ•°ï¼ˆé»˜è®¤ 0.1ï¼‰')
    p.add_argument('--dropout', type=float, default=0.5)
    p.add_argument('--use-focal', action='store_true', help='ä½¿ç”¨ FocalLossï¼ˆé»˜è®¤å…³é—­ï¼‰')
    p.add_argument('--focal-gamma', type=float, default=2.0)
    p.add_argument('--early-stop', type=int, default=5, help='EarlyStopping patience')
    p.add_argument('--clip-max-norm', type=float, default=1.0)
    p.add_argument('--seed', type=int, default=42)
    p.add_argument('--model-name', type=str, default='fine_tuned_model', help='ä¿å­˜åç§°ï¼ˆä¿å­˜åˆ° checkpoints/{name}.pthï¼‰')
    return p


def main():
    args = build_cli().parse_args()

    print("="*70)
    print("ğŸ”„ Transformer Direction Estimator - å¢é‡è®­ç»ƒ")
    print("="*70)

    if not HAS_TRANSFORMERS:
        print("\nâš ï¸ æœªå®‰è£… transformersï¼Œå°†æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨/Embeddingï¼špip install transformers")
        cont = input("æ˜¯å¦ç»§ç»­ï¼ˆå°†é€€å›å­—ç¬¦çº§Tokenizerï¼Œä¸å»ºè®®ï¼‰ï¼Ÿ[y/N]: ").strip().lower()
        if cont != 'y':
            print("å–æ¶ˆ")
            return

    if not os.path.exists(args.model):
        print(f"âŒ æ‰¾ä¸åˆ°åŸºç¡€æ¨¡å‹: {args.model}")
        return
    if not os.path.exists(args.input):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {args.input}")
        return

    cfg = {
        'bert_model': args.bert_model,
        'freeze_embedding': args.freeze_embedding,
        'max_length': args.max_length,
        'train_split': args.train_split,
        'batch_size': args.batch_size,
        'num_epochs': args.epochs,
        'learning_rate': args.lr,
        'ft_scale': args.ft_scale,
        'dropout': args.dropout,
        'use_focal': args.use_focal,
        'focal_gamma': args.focal_gamma,
        'early_stop': args.early_stop,
        'clip_max_norm': args.clip_max_norm,
        'seed': args.seed,
        'model_name': args.model_name,
    }

    print("\næœ¬æ¬¡å¾®è°ƒé…ç½®ï¼š")
    print("-"*70)
    for k, v in cfg.items():
        print(f"  {k}: {v}")
    print("-"*70)

    start = input("\nå¼€å§‹å¢é‡è®­ç»ƒï¼Ÿ[Y/n]: ").strip().lower()
    if start == 'n':
        print("å–æ¶ˆ")
        return

    try:
        fine_tune(args.model, args.input, cfg)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()