"""
å¢å¼ºç‰ˆè¯„ä¼°è„šæœ¬ - æ”¯æŒé€‰æ‹©æ¨¡å‹æ–‡ä»¶å’Œå¤šä¸ªCSVæ–‡ä»¶
"""

import torch
import pandas as pd
from mamba_de import MambaDirectionEstimator
from tqdm import tqdm
import argparse
import os
import numpy as np
import glob

try:
    from transformers import BertTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False


def list_model_files(directory='checkpoints'):
    """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹æ–‡ä»¶"""
    if not os.path.exists(directory):
        return []
    
    model_files = glob.glob(os.path.join(directory, '*.pth'))
    return sorted(model_files, key=os.path.getmtime, reverse=True)  # æŒ‰æ—¶é—´æ’åº


def list_csv_files(directory='.'):
    """åˆ—å‡ºæ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = glob.glob(os.path.join(directory, '*.csv'))
    return sorted(csv_files)


def select_model_interactive():
    """äº¤äº’å¼é€‰æ‹©æ¨¡å‹æ–‡ä»¶"""
    print("\n" + "="*70)
    print("ğŸ¤– é€‰æ‹©æ¨¡å‹æ–‡ä»¶")
    print("="*70)
    
    model_files = list_model_files()
    
    if not model_files:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ (åœ¨checkpoints/ç›®å½•)")
        model_path = input("\nè¯·è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„: ").strip()
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {model_path}")
        return model_path
    
    print("\næ‰¾åˆ°ä»¥ä¸‹æ¨¡å‹:")
    for i, file in enumerate(model_files, 1):
        size = os.path.getsize(file) / (1024 * 1024)  # MB
        mtime = os.path.getmtime(file)
        import time
        time_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(mtime))
        
        # å°è¯•è¯»å–å‡†ç¡®ç‡
        try:
            checkpoint = torch.load(file, map_location='cpu')
            acc = checkpoint.get('val_acc', 0)
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} MB, å‡†ç¡®ç‡: {acc:.2f}%, {time_str})")
        except:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} MB, {time_str})")
    
    print(f"  {len(model_files)+1}. æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„")
    
    while True:
        choice = input(f"\nè¯·é€‰æ‹© [1-{len(model_files)+1}]: ").strip()
        
        if not choice.isdigit():
            print("âŒ è¯·è¾“å…¥æ•°å­—")
            continue
        
        choice = int(choice)
        
        if 1 <= choice <= len(model_files):
            return model_files[choice-1]
        elif choice == len(model_files) + 1:
            model_path = input("è¯·è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(model_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {model_path}")
                continue
            return model_path
        else:
            print(f"âŒ è¯·è¾“å…¥1-{len(model_files)+1}ä¹‹é—´çš„æ•°å­—")


def select_multiple_csv(csv_files):
    """é€‰æ‹©å¤šä¸ªCSVæ–‡ä»¶"""
    print("\n" + "="*70)
    print("ğŸ“š å¤šCSVæ–‡ä»¶é€‰æ‹©")
    print("="*70)
    print("\næç¤ºï¼šè¾“å…¥æ•°å­—åºå·ï¼Œç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”")
    print("ä¾‹å¦‚: 1,2,3 æˆ– 1 2 3")
    print("è¾“å…¥ 'all' é€‰æ‹©å…¨éƒ¨\n")
    
    for i, file in enumerate(csv_files, 1):
        size = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB, ~{total_rows} è¡Œ)")
        except:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB)")
    
    while True:
        selection = input(f"\nè¯·é€‰æ‹©è¦é¢„æµ‹çš„CSVæ–‡ä»¶: ").strip()
        
        if selection.lower() == 'all':
            return csv_files
        
        try:
            if ',' in selection:
                indices = [int(x.strip()) for x in selection.split(',')]
            else:
                indices = [int(x.strip()) for x in selection.split()]
            
            selected_files = []
            for idx in indices:
                if 1 <= idx <= len(csv_files):
                    selected_files.append(csv_files[idx-1])
                else:
                    print(f"âŒ åºå· {idx} æ— æ•ˆ")
                    break
            else:
                if selected_files:
                    print(f"\nâœ“ å·²é€‰æ‹© {len(selected_files)} ä¸ªæ–‡ä»¶:")
                    for f in selected_files:
                        print(f"  - {os.path.basename(f)}")
                    return selected_files
                else:
                    print("âŒ æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶")
        except ValueError:
            print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å­—åºå·")


def select_csv_interactive():
    """äº¤äº’å¼é€‰æ‹©CSVæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰"""
    print("\n" + "="*70)
    print("ğŸ“ é€‰æ‹©è¯„ä¼°æ•°æ®")
    print("="*70)
    
    csv_files = list_csv_files()
    
    if not csv_files:
        print("âŒ å½“å‰ç›®å½•æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        csv_path = input("\nè¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
        return [csv_path]
    
    print("\næ‰¾åˆ°ä»¥ä¸‹CSVæ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        size = os.path.getsize(file) / 1024
        try:
            total_rows = sum(1 for _ in open(file, encoding='utf-8')) - 1
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB, ~{total_rows} è¡Œ)")
        except:
            print(f"  {i}. {os.path.basename(file):30s} ({size:.1f} KB)")
    
    print(f"  {len(csv_files)+1}. æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„")
    print(f"  {len(csv_files)+2}. é€‰æ‹©å¤šä¸ªCSVæ–‡ä»¶")
    
    while True:
        choice = input(f"\nè¯·é€‰æ‹© [1-{len(csv_files)+2}]: ").strip()
        
        if not choice.isdigit():
            print("âŒ è¯·è¾“å…¥æ•°å­—")
            continue
        
        choice = int(choice)
        
        if 1 <= choice <= len(csv_files):
            return [csv_files[choice-1]]
        elif choice == len(csv_files) + 1:
            csv_path = input("è¯·è¾“å…¥CSVæ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(csv_path):
                print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
                continue
            return [csv_path]
        elif choice == len(csv_files) + 2:
            return select_multiple_csv(csv_files)
        else:
            print(f"âŒ è¯·è¾“å…¥1-{len(csv_files)+2}ä¹‹é—´çš„æ•°å­—")


class Evaluator:
    """è¯„ä¼°å™¨"""
    
    def __init__(self, model_path):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.use_bert_tokenizer = checkpoint.get('use_bert_tokenizer', True)
        
        if self.use_bert_tokenizer:
            if not HAS_TRANSFORMERS:
                raise ImportError("éœ€è¦å®‰è£…: pip install transformers")
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        else:
            self.char_to_idx = checkpoint['char_to_idx']
        
        self.model = MambaDirectionEstimator(
            vocab_size=checkpoint['vocab_size'],
            d_model=checkpoint['d_model'],
            d_state=checkpoint['d_state']
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model = self.model.to(self.device)
        self.model.eval()
        
        self.max_length = 64
        
        self.direction_map = {0: 'left', 1: 'right', 2: 'bidirectional'}
        self.direction_names = {0: 'å·¦å‘(å› æœ)', 1: 'å³å‘(åå› æœ)', 2: 'åŒå‘'}
        
        self.label_to_idx = {
            'left': 0, 'right': 1, 'bidirectional': 2,
            'å·¦': 0, 'å³': 1, 'åŒå‘': 2,
            'L': 0, 'R': 1, 'B': 2
        }
        
        tokenizer_type = "BERT" if self.use_bert_tokenizer else "å­—ç¬¦çº§"
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½ (Tokenizer: {tokenizer_type}, è®¾å¤‡: {self.device})")
        print(f"âœ“ éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']:.2f}%")
    
    def tokenize(self, text: str):
        if self.use_bert_tokenizer:
            encoded = self.tokenizer.encode(
                text,
                add_special_tokens=False,
                max_length=self.max_length,
                truncation=True,
                padding='max_length'
            )
            return torch.tensor([encoded], dtype=torch.long)
        else:
            tokens = [self.char_to_idx.get(char, 1) for char in text[:self.max_length]]
            if len(tokens) < self.max_length:
                tokens = tokens + [0] * (self.max_length - len(tokens))
            return torch.tensor([tokens], dtype=torch.long)
    
    def predict_one(self, text):
        input_ids = self.tokenize(text).to(self.device)
        
        with torch.no_grad():
            directions, probs = self.model.predict(input_ids)
        
        pred_idx = directions[0].item()
        
        return {
            'pred_idx': pred_idx,
            'pred_label': self.direction_map[pred_idx],
            'pred_label_cn': self.direction_names[pred_idx],
            'confidence': float(probs[0][pred_idx]),
            'prob_left': float(probs[0][0]),
            'prob_right': float(probs[0][1]),
            'prob_bi': float(probs[0][2])
        }
    
    def evaluate_csv(self, input_path, output_path, text_col='text', label_col='direction'):
        """è¯„ä¼°å•ä¸ªCSVæ–‡ä»¶"""
        print(f"\n{'='*70}")
        print(f"è¯„ä¼°æ–‡ä»¶: {os.path.basename(input_path)}")
        print(f"{'='*70}")
        
        df = pd.read_csv(input_path, encoding='utf-8')
        
        if text_col not in df.columns:
            raise ValueError(f"æ‰¾ä¸åˆ°æ–‡æœ¬åˆ— '{text_col}'")
        
        has_labels = label_col in df.columns
        
        print(f"âœ“ åŠ è½½äº† {len(df)} æ¡æ ·æœ¬")
        
        if has_labels:
            true_labels_idx = []
            true_labels_cn = []
            
            for label in df[label_col]:
                label_str = str(label).strip()
                true_idx = self.label_to_idx.get(label_str, -1)
                if true_idx == -1:
                    true_labels_cn.append("æœªçŸ¥")
                else:
                    true_labels_cn.append(self.direction_names[true_idx])
                true_labels_idx.append(true_idx)
            
            df['true_label_idx'] = true_labels_idx
            df['true_label_cn'] = true_labels_cn
            
            valid_df = df[df['true_label_idx'] != -1].copy()
            if len(valid_df) < len(df):
                print(f"âš ï¸  è¿‡æ»¤æ‰ {len(df) - len(valid_df)} æ¡æ— æ•ˆæ ‡ç­¾")
            
            print(f"âœ“ æœ‰æ•ˆæ ·æœ¬: {len(valid_df)} æ¡")
        else:
            print("â„¹ï¸  æ— æ ‡ç­¾åˆ—ï¼Œä»…è¿›è¡Œé¢„æµ‹")
            valid_df = df.copy()
        
        print("\nå¼€å§‹é¢„æµ‹...")
        predictions = []
        
        for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df), desc="é¢„æµ‹è¿›åº¦"):
            text = str(row[text_col])
            result = self.predict_one(text)
            predictions.append(result)
        
        valid_df['pred_label'] = [p['pred_label'] for p in predictions]
        valid_df['pred_label_cn'] = [p['pred_label_cn'] for p in predictions]
        valid_df['pred_idx'] = [p['pred_idx'] for p in predictions]
        valid_df['confidence'] = [p['confidence'] for p in predictions]
        valid_df['prob_left'] = [p['prob_left'] for p in predictions]
        valid_df['prob_right'] = [p['prob_right'] for p in predictions]
        valid_df['prob_bidirectional'] = [p['prob_bi'] for p in predictions]
        
        if has_labels:
            valid_df['correct'] = valid_df['true_label_idx'] == valid_df['pred_idx']
            valid_df['result'] = valid_df['correct'].apply(lambda x: 'âœ“ æ­£ç¡®' if x else 'âœ— é”™è¯¯')
        
        valid_df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        if has_labels:
            self.print_evaluation(valid_df)
        else:
            self.print_prediction_summary(valid_df)
        
        return valid_df
    
    def print_evaluation(self, df):
        """æ‰“å°è¯„ä¼°ç»“æœï¼ˆæœ‰æ ‡ç­¾ï¼‰"""
        print("\n" + "="*70)
        print("è¯„ä¼°ç»“æœ")
        print("="*70)
        
        accuracy = (df['correct'].sum() / len(df)) * 100
        print(f"\næ€»ä½“å‡†ç¡®ç‡: {accuracy:.2f}% ({df['correct'].sum()}/{len(df)})")
        
        print(f"å¹³å‡ç½®ä¿¡åº¦: {df['confidence'].mean():.2%}")
        if df['correct'].sum() > 0:
            print(f"  æ­£ç¡®é¢„æµ‹çš„ç½®ä¿¡åº¦: {df[df['correct']]['confidence'].mean():.2%}")
        if (~df['correct']).sum() > 0:
            print(f"  é”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦: {df[~df['correct']]['confidence'].mean():.2%}")
        
        print("\næ–¹å‘åˆ†å¸ƒ:")
        direction_counts = df['pred_label_cn'].value_counts()
        for direction, count in direction_counts.items():
            pct = 100 * count / len(df)
            print(f"  {direction}: {count} ({pct:.1f}%)")
    
    def print_prediction_summary(self, df):
        """æ‰“å°é¢„æµ‹æ‘˜è¦ï¼ˆæ— æ ‡ç­¾ï¼‰"""
        print("\n" + "="*70)
        print("é¢„æµ‹ç»“æœæ‘˜è¦")
        print("="*70)
        
        print(f"\næ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {df['confidence'].mean():.2%}")
        
        print("\né¢„æµ‹æ–¹å‘åˆ†å¸ƒ:")
        direction_counts = df['pred_label_cn'].value_counts()
        for direction, count in direction_counts.items():
            pct = 100 * count / len(df)
            print(f"  {direction}: {count} ({pct:.1f}%)")


def interactive_mode(evaluator):
    """äº¤äº’æ¨¡å¼"""
    print("\n" + "="*70)
    print("ğŸ¯ äº¤äº’å¼é¢„æµ‹æ¨¡å¼")
    print("="*70)
    print("è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ 'quit' æˆ– 'q' é€€å‡º")
    print("-"*70)
    
    while True:
        text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
        
        if text.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        
        if not text:
            print("âš ï¸  æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
        
        try:
            result = evaluator.predict_one(text)
            
            print("\n" + "="*70)
            print("ğŸ“Š é¢„æµ‹ç»“æœ")
            print("="*70)
            print(f"æ–‡æœ¬: {text}")
            print(f"\nğŸ¯ é¢„æµ‹æ–¹å‘: {result['pred_label_cn']} ({result['pred_label']})")
            print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {result['confidence']:.2%}")
            print(f"\nè¯¦ç»†æ¦‚ç‡åˆ†å¸ƒ:")
            print(f"  å·¦å‘(å› æœ):      {result['prob_left']:.2%} {'â–ˆ' * int(result['prob_left'] * 30)}")
            print(f"  å³å‘(åå› æœ):    {result['prob_right']:.2%} {'â–ˆ' * int(result['prob_right'] * 30)}")
            print(f"  åŒå‘:            {result['prob_bi']:.2%} {'â–ˆ' * int(result['prob_bi'] * 30)}")
            print("="*70)
            
        except Exception as e:
            print(f"\nâŒ é¢„æµ‹å‡ºé”™: {e}")


def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆè¯„ä¼° - æ”¯æŒé€‰æ‹©æ¨¡å‹å’Œå¤šCSVæ–‡ä»¶')
    parser.add_argument('-m', '--model', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-i', '--input', type=str, nargs='+', help='è¾“å…¥CSVæ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰')
    parser.add_argument('-o', '--output-dir', type=str, default='.', help='è¾“å‡ºç›®å½•')
    parser.add_argument('-t', '--text-col', type=str, default='text', help='æ–‡æœ¬åˆ—å')
    parser.add_argument('-l', '--label-col', type=str, default='direction', help='æ ‡ç­¾åˆ—å')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸ¯ Mamba Direction Estimator - å¢å¼ºç‰ˆè¯„ä¼°å·¥å…·")
    print("="*70)
    
    # 1. é€‰æ‹©æ¨¡å‹
    if args.model:
        model_path = args.model
        if not os.path.exists(model_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            return
    else:
        model_path = select_model_interactive()
    
    # 2. åŠ è½½æ¨¡å‹
    try:
        evaluator = Evaluator(model_path)
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # 3. é€‰æ‹©CSVæ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯äº¤äº’æ¨¡å¼ï¼‰
    if not args.interactive:
        if args.input:
            csv_paths = args.input
            for csv_path in csv_paths:
                if not os.path.exists(csv_path):
                    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")
                    return
        else:
            csv_paths = select_csv_interactive()
        
        # 4. æ‰¹é‡è¯„ä¼°
        os.makedirs(args.output_dir, exist_ok=True)
        
        all_results = []
        for csv_path in csv_paths:
            output_name = os.path.basename(csv_path).replace('.csv', '_predicted.csv')
            output_path = os.path.join(args.output_dir, output_name)
            
            try:
                df = evaluator.evaluate_csv(
                    input_path=csv_path,
                    output_path=output_path,
                    text_col=args.text_col,
                    label_col=args.label_col
                )
                all_results.append({
                    'file': os.path.basename(csv_path),
                    'samples': len(df),
                    'output': output_path
                })
            except Exception as e:
                print(f"\nâŒ å¤„ç† {csv_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # 5. æ€»ç»“
        if len(csv_paths) > 1:
            print("\n" + "="*70)
            print("ğŸ“Š æ‰¹é‡è¯„ä¼°å®Œæˆ")
            print("="*70)
            for result in all_results:
                print(f"\næ–‡ä»¶: {result['file']}")
                print(f"  æ ·æœ¬æ•°: {result['samples']}")
                print(f"  è¾“å‡º: {result['output']}")
    
    # 6. äº¤äº’æ¨¡å¼
    if args.interactive or not args.input:
        interactive_mode(evaluator)


if __name__ == '__main__':
    main()